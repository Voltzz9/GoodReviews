{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bab3fc-d83f-40fd-8ca4-8590f72b3114",
   "metadata": {},
   "source": [
    "# Data Science 346 Project Stellenbosch University\n",
    "### Team:\n",
    "- David Nicolay 26296918\n",
    "- Kellen Mossner 26024284\n",
    "- Matthew Holm 26067404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8157126f-431f-448c-94bb-4b8464043d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set Random seed for reproducible results\n",
    "random_seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a029d21-988a-4555-8819-97ffab1b54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/davidnicolay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/davidnicolay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/davidnicolay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK packages downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caea788-fb05-4751-87eb-22194b033b23",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281542c-d91f-41d8-83e9-93a9156093f7",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2703d11d-77df-4cac-be67-7281b193669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Review Stars</th>\n",
       "      <th>Review Likes</th>\n",
       "      <th>Genres</th>\n",
       "      <th>First Published Date</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>This book is based on a television series whic...</td>\n",
       "      <td>September 29, 2014</td>\n",
       "      <td>5</td>\n",
       "      <td>513</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>I am not the audience for this book, mainly be...</td>\n",
       "      <td>June 3, 2014</td>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>Way of Seeing, John Berger Ways of Seeing is a...</td>\n",
       "      <td>October 21, 2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>First of all, this entire book is set in bold....</td>\n",
       "      <td>May 25, 2008</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>This was a great introduction to the work of J...</td>\n",
       "      <td>March 12, 2020</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Book Title                                               Link  \\\n",
       "0  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "1  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "2  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "3  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "4  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "\n",
       "                                         Review Text         Review Date  \\\n",
       "0  This book is based on a television series whic...  September 29, 2014   \n",
       "1  I am not the audience for this book, mainly be...        June 3, 2014   \n",
       "2  Way of Seeing, John Berger Ways of Seeing is a...    October 21, 2021   \n",
       "3  First of all, this entire book is set in bold....        May 25, 2008   \n",
       "4  This was a great introduction to the work of J...      March 12, 2020   \n",
       "\n",
       "   Review Stars  Review Likes  \\\n",
       "0             5           513   \n",
       "1             3           216   \n",
       "2             4             0   \n",
       "3             4           106   \n",
       "4             4            80   \n",
       "\n",
       "                                              Genres First Published Date  \\\n",
       "0  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "1  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "2  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "3  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "4  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "\n",
       "        Author  \n",
       "0  John Berger  \n",
       "1  John Berger  \n",
       "2  John Berger  \n",
       "3  John Berger  \n",
       "4  John Berger  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "reviews = pd.read_csv(\"../WebScrapingExplore/data/goodreads_reviews_all.csv\")\n",
    "book_genres = pd.read_csv(\"../WebScrapingExplore/data/book_info.csv\")\n",
    "\n",
    "reviews = pd.merge(reviews, book_genres, on='Book Title', how='inner')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4d9244-2fe3-4501-8941-9e2d77736ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35519 entries, 0 to 35518\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Book Title            35519 non-null  object\n",
      " 1   Link                  35519 non-null  object\n",
      " 2   Review Text           35150 non-null  object\n",
      " 3   Review Date           35519 non-null  object\n",
      " 4   Review Stars          35519 non-null  int64 \n",
      " 5   Review Likes          35519 non-null  int64 \n",
      " 6   Genres                35519 non-null  object\n",
      " 7   First Published Date  35519 non-null  object\n",
      " 8   Author                35519 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb9d81-b081-4986-bcd9-a7a06bf58b86",
   "metadata": {},
   "source": [
    "Despite us filtering only english reviews when scraping, some reviews tagged as english are still written in various other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0884873-89a2-40dc-bfc4-0372e2c0a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language\n",
    "def is_english(review):\n",
    "    try:\n",
    "        return detect(review) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0154a7e1-b5dd-47d8-be52-65d8a7316a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only English reviews- This may take a while\n",
    "reviews = reviews[reviews['Review Text'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92630c16-ffb5-47ee-8263-d366d0a4607f",
   "metadata": {},
   "source": [
    "# Part 1: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01b48e-9643-47bc-adde-ed87e63ef940",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9c880-0873-41fe-b259-8df4176ab9be",
   "metadata": {},
   "source": [
    "Initializing the pipeline will take a while to run at first, since this function downloads the model weights (about 1.6gb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbd3b63-8eeb-4846-a70b-6e29e4dab7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbad676-5114-4c67-9fb7-147d0a612722",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf2f02-1f0c-4c0b-b779-528fb3497d0d",
   "metadata": {},
   "source": [
    "Due to restricted input length of the summarizer the reviews text needs to be divided into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62176a75-3abc-4304-adc1-55410253691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    for word in words:\n",
    "        if current_size + len(word) > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_size = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_size += len(word) + 1  # +1 for space\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, max_summary_length=150):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=max_summary_length, min_length=10)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    final_summary = ' '.join(summaries)\n",
    "    if len(final_summary) > max_summary_length:\n",
    "        final_summary = summarizer(final_summary, max_length=max_summary_length, min_length=30)[0]['summary_text']\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6df7e-0359-45a4-9b50-40ff115e4650",
   "metadata": {},
   "source": [
    "Begin by summarizing 1 book's reviews - \"Ways of Seeing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba206aa4-bb28-4c99-9aa5-668b6f98e2ca",
   "metadata": {},
   "source": [
    "Here we can have a look at how the model does a good job of summarizing (but it essentially picks important sentences), however we still need to present it in a format that explains the overall sentiment from the readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c2f252-4952-477d-8184-ea5ec09ca297",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test = summarizer(reviews.loc[3]['Review Text'], max_length=50, min_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19feaaa1-c1b0-4ce8-a7ef-459ae795c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '4 essays and 3 pictorial essays. It seems like museums are doing a lot of things wrong as well as right. Chapter on oil-painting was particularly interesting but it was the last one about advertising (or \"publicity\"'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6de8201a-f1e9-4586-af53-211c09f377c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First of all, this entire book is set in bold. I don\\'t know what crazy crazyman let that through the gate at Penguin but I just felt I had to point it out right away. It\\'s still worth reading. 4 essays and 3 pictorial essays. Really interesting stuff cutting away some of the bullshit associated with our appreciation of art. It seems like museums are doing a lot of things wrong as well as right. Chapter on oil-painting was particularly interesting but it was the last one about advertising (or \"publicity\" as it\\'s exclusively referred to in this book) that has me thinking. Advertising not only needs you to want this shirt, this car, the entire industry must endeavor to narrow the scope of your desires to make you amenable to the culture. The mindset must always be a future, better you achieved through important purchases. The essay is horrifying enough until you realise that it\\'s thirty years old, and this is now only one facet of a business that\\'s grown much more insidious. The ads shown are almost quaint in their straight sell.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.loc[3]['Review Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb0cbf-70c2-4b49-815a-870eb2429542",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb12e3b-d37c-434f-a750-590ac14f3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************************\n",
    "def preprocess(reviews):\n",
    "    \"\"\"\n",
    "    Replaces newline characters with spaces\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    print(f\"Number of reviews: {n_reviews}\")\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        # Replace newlines with spaces\n",
    "        reviews[i] = review.replace('\\n', ' ').strip()\n",
    "        \n",
    "        \n",
    "def split_sentences(reviews):\n",
    "    \"\"\"\n",
    "    Splits the reviews into individual sentences.\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        # import nltk\n",
    "        # nltk.download('punkt')\n",
    "        sentences = sent_tokenize(review)  # Tokenize into sentences\n",
    "        # Remove empty sentences and strip spaces\n",
    "        sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        reviews[i] = sentences\n",
    "        \n",
    "        \n",
    "def encode_sentences(reviews):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the reviews\n",
    "    using Sentence-BERT from the sentence-transformers library.\n",
    "    \"\"\"\n",
    "    enc_reviews = [None] * len(reviews)\n",
    "    cum_sum_sentences = [0]\n",
    "    sent_count = 0\n",
    "    \n",
    "    # Flatten reviews into a list of sentences\n",
    "    for review in reviews:\n",
    "        sent_count += len(review)\n",
    "        cum_sum_sentences.append(sent_count)\n",
    "\n",
    "    all_sentences = [sent for review in reviews for sent in review]\n",
    "    print('Loading pre-trained Sentence-BERT model...')\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "    # Group back encoded sentences by reviews\n",
    "    for i in range(len(reviews)):\n",
    "        begin = cum_sum_sentences[i]\n",
    "        end = cum_sum_sentences[i+1]\n",
    "        enc_reviews[i] = enc_sentences[begin:end]\n",
    "        \n",
    "    return enc_reviews\n",
    "        \n",
    "    \n",
    "def summarize(reviews):\n",
    "    \"\"\"\n",
    "    Performs summarization of book reviews.\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    summary = [None] * n_reviews\n",
    "    print('Preprocessing...')\n",
    "    preprocess(reviews)\n",
    "    \n",
    "    print('Splitting into sentences...')\n",
    "    split_sentences(reviews)\n",
    "    \n",
    "    print('Starting to encode...')\n",
    "    enc_reviews = encode_sentences(reviews)\n",
    "    print('Encoding Finished')\n",
    "    \n",
    "    for i in range(n_reviews):\n",
    "        enc_review = enc_reviews[i]\n",
    "        n_clusters = int(np.ceil(len(enc_review) ** 0.5))  # Number of clusters\n",
    "        \n",
    "        # Perform KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "        kmeans = kmeans.fit(enc_review)\n",
    "        \n",
    "        avg = []\n",
    "        closest = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_review)\n",
    "        \n",
    "        # Ordering sentences by clusters\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        summary[i] = ' '.join([reviews[i][closest[idx]] for idx in ordering])\n",
    "    \n",
    "    print('Clustering Finished')\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab984d2a-8178-4f5a-92da-f0a4b3fc6a0a",
   "metadata": {},
   "source": [
    "#### What is happening here?\n",
    "Encoding:\n",
    "- Sentence-BERT: The function uses a pre-trained Sentence-BERT model ('all-MiniLM-L6-v2') to convert each sentence into a vector embedding. - This embedding is a numerical representation of the sentence that captures its semantic meaning.\n",
    "- It first flattens all the sentences from the reviews into a single list and then encodes them.\n",
    "- After encoding, it restructures the embeddings back into their original review groups.\n",
    "\n",
    "Clustering:\n",
    "- For each review, the number of clusters is determined using the square root of the number of sentences (rounded up).\n",
    "- KMeans clustering is performed on the sentence embeddings to group similar sentences.\n",
    "- For each cluster, **the sentences closest to the cluster center (based on the distance between sentence embeddings) are selected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a344d-e052-411e-b037-9b719a5b1e68",
   "metadata": {},
   "source": [
    "### Applying to a single book's review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2739a959-7643-41b1-9e23-2e21c952772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_subset = reviews[reviews['Book Title'] == 'Ways of Seeing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0363ed2-5433-4a0a-9d14-59d2ee3f7dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 117\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc2777b7ef74dcc8111267cffe25145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    }
   ],
   "source": [
    "way_of_seeing_reviews = reviews_subset['Review Text'].tolist()\n",
    "summaries = summarize(way_of_seeing_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c19353f4-f1bd-424e-9468-9391ce4ad9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In general, the lesson of this book is that all art is bad for you, except the pieces that the authors of this book like. I am not the audience for this book, mainly because I've already read and more or less digested the handful of essays and ideas on which it is based. This is true. They don't discuss the 20th century at all (I know they know that twentieth century art exists; perhaps, as good Benjaminian Marxists, they don't like abstraction or difficulty). Holbein's 'Ambassadors' is read as an example of this; the incredible distorted skull in the painting is the exception which proves the rule of oil paintings rather than, you know, showing that oil paintings can be self-critical, as are most good artworks of any kind. Good for what it is, but extremely narrow in scope, and quite harmful for anyone who swallows it whole rather than taking a few minutes to worry away at its assumptions.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3928fac9-10b9-46fc-bc7b-98d18b378739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not the audience for this book, mainly because I\\'ve already read and more or less digested the handful of essays and ideas on which it is based. The seven chapters break down fairly simply. 1: Benjamin\\'s \\'Work of Art\\'--the ability to reproduce images alters the way we encounter works of art. This seems reasonable. Nobody gets to see a Giotto without having seen a reproduction first, except someone who has no interest in the Giotto in the first place. But Berger et al* go a step further: we need to use the fact that we encounter works of art differently to undermine the ruling class\\'s privilege and the \"specialized experts who are the clerks of the nostalgia of a ruling class in decline.\" That\\'s on page 32. Part of me, a large part, laments the fact that you\\'d never get that published today, not even on a website. Another part of me laments the stupidity of intellectuals who put their faith in the inherent goodness of The People. The People does not have a good track record when it comes to art appreciation. That\\'s not to say that people can\\'t learn to appreciate art, only that We are no better and no worse than the ruling class was. We need to learn, we need to be taught, you can\\'t do that if you assume that We are inherently able to do the right thing. 2 & 3: Women are depicted differently from men, and, frankly, not in ways that are healthy for anyone, but particularly not for women. I agree. Which makes it breathtaking to see the authors get so many things wrong, either intentionally (cutting short the bible verse in which God punishes Eve *and Adam*); stupidly (non-Western art forms show women as active participants in sex, so that are isn\\'t morally dubious); or in ways that are, ahem, temporally bound (\"Hair is associated with sexual power, with passion.\" Seventies!). 5: Oil paintings are bourgeois and generally not morally okay. Holbein\\'s \\'Ambassadors\\' is read as an example of this; the incredible distorted skull in the painting is the exception which proves the rule of oil paintings rather than, you know, showing that oil paintings can be self-critical, as are most good artworks of any kind. In general, the lesson of this book is that all art is bad for you, except the pieces that the authors of this book like. They like pieces by artists who can plausibly be turned into radicals, because only radicals can be interesting (Franz Hals; William Blake). They don\\'t discuss the 20th century at all (I know they know that twentieth century art exists; perhaps, as good Benjaminian Marxists, they don\\'t like abstraction or difficulty). They\\'re also very uncomfortable with religious art, and want to group, e.g., Ambrosius Benson\\'s Mary Magdalene with the absurd and/or pornographic Magdalene of later times, rather than admitting the rather obvious differences (Benson\\'s is rich, but not, how can I put this... naked and disheveled.) Since the authors have a hard time saying what they actually like (vs. what they suspect is oppressive), you get idiocies like this: Rembrandt\\'s famous late portrait shows a man for whom \"all has gone except a sense of the question of existence, of existence as a question.\" A little thought would show that this is the sort of conservative pablum Great Artists have been serving up for generations. 6 & 7: Advertizing uses art to make you think you want things you don\\'t want and that you can get them, so you don\\'t need to think about what you really want, e.g., more time away from the office. This is true. In sum: I was sucked in by the idea that this was a book about understanding art. It is not. It is critical theory for high-school readers. Good for what it is, but extremely narrow in scope, and quite harmful for anyone who swallows it whole rather than taking a few minutes to worry away at its assumptions. Harmful because those who accept it will say silly things, and because those who read it and reject it out of hand (due to the rhetoric, bad arguments, or conceptual confusion) won\\'t be challenged to, you know, care about other people. * Humorous aspect of this book: it makes a big deal about how it was written by a group of people, because, you know, individuals are bad, and groups are good. You\\'ll note that the book is sold as a book by John Berger. You can draw the conclusion.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Review Text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9120079-d956-4cbd-9952-acb1f8a738c0",
   "metadata": {},
   "source": [
    "## Full Book Review Summary Generation\n",
    "Now with the ability to create summaries for each review individually, we can generate one last summary - one that describes how most readers perceive the book (themes, flaws, strengths, etc). First, let's generate another set of summarized reviews.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b778a-c6b7-4cbc-b5d6-a42cd1a8d6c0",
   "metadata": {},
   "source": [
    "#### Applying Review Summaries to Born A Crime by Trevor Noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c2c6f4-62c7-49e0-a007-ded060e261a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea57c77ce7492095ce750b0b6c3ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    }
   ],
   "source": [
    "born_a_crime_subset = reviews[reviews['Book Title'] == 'Born a Crime: Stories From a South African Childhood']\n",
    "born_a_crime_reviews = born_a_crime_subset['Review Text'].tolist()\n",
    "born_a_crime_summaries = summarize(born_a_crime_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1f3afc-c7af-4773-b48b-d4d3b63401d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noah is very wise to be so young, one of my favorite quotes from the book is \"The hood was strangely comforting, but comfort can be dangerous. There were different rules for whites, colored (people descended from the first white settlers and the natives), blacks, Indians and Asians. One of the many things I didn\\'t know about South Africa is that as most countries were trying to fix racist policies after the World War 2 holocaust had shown us where discrimination could lead, the South Africans or Afrikaners (as the Dutch colonists called themselves) were running towards institutionalized racism. Comfort provides a floor but also a ceiling.\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_summaries[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b960995d-e9d6-4593-b3e4-f50d9997c731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the best memoirs I've ever read, Born a Crime is so funny and wise and sad and reveals much about the writer.\",\n",
       " \"Trevor Noah is an exceptional young man raised by a strong, formidable mother who never gave up on him and loved him with a fierce pride.He never shrinks from complete honesty in the telling, even in those areas that don't reflect well on his teenaged and early adult self.\",\n",
       " 'One very amusing incident tells of how as a kid his rejection of going to the outside toilet in the rain led to a most unfortunate incident with his blind grandma.',\n",
       " \"Like in the best books, you learn some important history in the telling of his young life's story.\",\n",
       " 'Apartheid was made officially part of South African government in 1948; whereas, before you had casual, implied racism, now it was a system of specific laws that rated you as a person.',\n",
       " 'There were different rules for whites, colored (people descended from the first white settlers and the natives), blacks, Indians and Asians.',\n",
       " 'Blacks were at the bottom with prison time for those who produced children from a black and white union.',\n",
       " 'Hence the title, Born a Crime, is stating a fact of life in the nation of South Africa at that time.',\n",
       " \"One of the many things I didn't know about South Africa is that as most countries were trying to fix racist policies after the World War 2 holocaust had shown us where discrimination could lead, the South Africans or Afrikaners (as the Dutch colonists called themselves) were running towards institutionalized racism.\",\n",
       " 'The Afrikaners were fans of Hitler and actually formed a committee to study racism around the world and pick out the\"best\" bits for themselves.',\n",
       " 'They studied Australia, the Netherlands and the US.',\n",
       " 'Noah sums it up very well: in America we had 1) forced removal of the native population to reservations, 2) slavery, followed by 3) segregation.',\n",
       " 'In South Africa all 3 were done at the same time to the same people.',\n",
       " 'Noah is very wise to be so young, one of my favorite quotes from the book is \"The hood was strangely comforting, but comfort can be dangerous.',\n",
       " 'Comfort provides a floor but also a ceiling.\"',\n",
       " \"This man's talent knows no ceiling.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_reviews[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a6b93-2b67-4841-a290-fca9cece6188",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So far so good - the summaries skip over some information here and there, but at a glance, they look quite accurate. Now let's take it a step further and generate a final summary that should encapsulate the core themes and ideas mentioned in the reviews.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5ac854-854a-4031-a317-a3bd7144e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_final(all_summaries):\n",
    "    \"\"\"\n",
    "    Performs summarization of the combined summaries.\n",
    "    \"\"\"\n",
    "    all_summaries = [summary.replace('\\n', ' ').strip() for summary in all_summaries]\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    all_sentences = []\n",
    "    for summary in all_summaries:\n",
    "        sentences = sent_tokenize(summary)\n",
    "        sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        all_sentences.extend(sentences)\n",
    "    \n",
    "    print('Loading pre-trained Sentence-BERT model...')\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "    n_clusters = int(np.ceil(len(enc_sentences) ** 0.5))\n",
    "    print('Clustering sentences...')\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(enc_sentences)\n",
    "    \n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_sentences)\n",
    "    \n",
    "    ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "    final_summary = ' '.join([all_sentences[closest[idx]] for idx in ordering])\n",
    "    \n",
    "    print('Final summary generated')\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2873e671-cec5-4b06-80f0-48c2867e0ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Sentence-BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57adcf02a9e84c0dbc86afe275f5c0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    }
   ],
   "source": [
    "final_born_a_crime_summary = summarize_final(born_a_crime_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdb17962-acfb-4ddd-8e8d-17d075a05ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I only usually have audiobooks on in the car or while cleaning, but for this I made time to have it on, I wanted to get back to the story. Noah's love and respect for his mother & the way she raised him shines through on nearly every page. I am a big fan of Trevor and watched his special on Netflix where he speaks of his life growing up but this goes into so much detail, it is stunning all that this guy goes through and is not a bitter man. As a reader, I appreciated the emphasis on his childhood. While Noah may just be a generally bubbly and optimistic person - which I respect a lot, considering the hardships he's overcome - there were chapters in which I wanted more introspection, more nuance, and more emotional vulnerability. Yet he tells his story with such humor and through such humanistic/existentialist perspective that one is left with awe and inspiration and deep respect for him. Shocking. I might not like that she believed in thrashing her children as punishment (she changed her opinion in the end) or that she stayed with a drunkard who abused her and the kids, but she is one hell of a smart and strong lady! The book is well written and delves into very important topics told in a funny way and the way it ended was just perfect. reading this made me realize that i am grievously uneducated on apartheid. Imagine being born from a black mother and a white father in a country where interracial relationships were against the law. One of the best audiobooks I've heard in a long time. I didn't know much about Trevor Noah other than the fact that he was a comedian before coming to that television show. I was lucky to see Trevor Noah speak about this book recently, and the way he talked about his story, and his life growing up in South Africa made me all the more eager to read it! His mother was a strong, independent woman in a place that didn’t appreciate or encourage that. This was fantastic. Throughout the troubles and strife, Trevor also weaves delightful, captivating stories about his own childhood and the experiences he had whilst growing up as an illegal biracial child in South Africa. “We tell people to follow their dreams, but you can only dream of what you can imagine, and, depending on where you come from, your imagination can be quite limited.” You know I am not the most generous with 5 stars. I thought he was funny and smart. i actually forgot i was reading a memoir because of how similar my feelings were when i read fiction. I recommend this book to absolutely everyone, I got sooo much more than I expected from it. It was fascinating learning about his life growing up as a mixed race child in pre- and post-Apartheid South Africa. !\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_born_a_crime_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9e95f-8275-4ed7-85da-2135d4a7f06e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This final generated summary mentions essentially every aspect that you can find in most reviews (audiobook being better, Noah's relationship with his mother, apartheid and the disconnect that some readers have with it, etc). \n",
    "\n",
    "---\n",
    "\n",
    "For the sake of coherency, we can now parse this final summary to `facebook/bart-large-cnn` with a prompt to reframe the summary as a book description.\n",
    "\n",
    "`facebook/bart-large-cnn` is a fine-tuned version of BART (Bidirectional and Auto-Regressive Transformer), a transformer-based sequence-to-sequence model introduced by Facebook AI, combining the advantages of bidirectional and auto-regressive models. The model uses an encoder-decoder architecture: the encoder processes the entire input (like BERT), and the decoder generates output token-by-token (like GPT).\n",
    "\n",
    "Pretrained on various denoising tasks to learn language patterns, BART was fine-tuned specifically on the CNN/DailyMail dataset to improve its summarization capabilities. During fine-tuning, the decoder attends to the encoder's output to generate contextually appropriate summaries (Attention Mechanisms). Beam search and length penalty were also used to avoid overly short summaries.\n",
    "\n",
    "The weights were initialized from the original BART model that was pretrained on a large corpus of text using the denoising autoencoding tasks mentioned earlier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d617e5b9-d7bd-45da-9573-655ac5865541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_book_description(summary):\n",
    "    \"\"\"\n",
    "    Takes the final summary of reviews and generates a book description.\n",
    "    \"\"\"\n",
    "    description_pipeline = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "    prompt = (\n",
    "        \"Convert the following summary into a structured book description that describes the book's content, \"\n",
    "        \"themes, and significance: \" + summary\n",
    "    )\n",
    "\n",
    "    result = description_pipeline(prompt, max_length=250, min_length=100, do_sample=False)\n",
    "\n",
    "    book_description = result[0]['generated_text']\n",
    "    #print(book_description)\n",
    "    return book_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42560d2c-6553-4d1b-a2a3-c32329e09bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "book_description = generate_book_description(final_born_a_crime_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5f30216-7aeb-4e4d-a1b6-ad65ea2d5af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the best audiobooks I've heard in a long time. Noah's love and respect for his mother & the way she raised him shines through on nearly every page. It was fascinating learning about his life growing up as a mixed race child in pre- and post-Apartheid South Africa. “We tell people to follow their dreams, but you can only dream of what you can imagine, and, depending on where you come from, your imagination can be quite limited.”\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718baf32-c6d4-4668-9da0-31ccf9114396",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can now apply this to any book we like to obtain an overview/description of the contents and themes soley based off of the reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26b08b-e114-4ee6-b6a5-510971b601e1",
   "metadata": {},
   "source": [
    "## Generating Final Summaries For 10 Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14c25d3b-5449-4a00-90ea-93db13b08ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lorax\n",
      "This Present Darkness\n",
      "Principles: Life and Work\n",
      "The Meaning of Marriage: Facing the Complexities of Commitment with the Wisdom of God\n",
      "Desiring God: Meditations of a Christian Hedonist\n",
      "Becoming\n",
      "The Diary of a Young Girl\n",
      "Yes Please\n",
      "Forgotten God: Reversing Our Tragic Neglect of the Holy Spirit\n",
      "The Hiding Place: The Triumphant True Story of Corrie Ten Boom\n",
      "The Great Divorce\n",
      "Matilda\n",
      "A Light in the Attic\n",
      "The Everything Store: Jeff Bezos and the Age of Amazon\n",
      "What Are You Looking At?: 150 Years of Modern Art in a Nutshell\n",
      "Pippi Longstocking\n",
      "The Lion, the Witch and the Wardrobe\n",
      "Alexander Hamilton\n",
      "The Secret Lives of Color\n",
      "Leonardo da Vinci\n",
      "\n",
      "\n",
      "Preprocessing...\n",
      "Number of reviews: 113\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681e435e2d664456bc9028345594e42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6311d70a2f441f964a06756e1caf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca2f2dc9e3d4ec69804bd0c0d317145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ccc1ce0301469989da96d8b86af1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 117\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53be916af03a43a3824da8f0d57b6874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57a161afd4400bb01b7a312009b1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidnicolay/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     book_summaries \u001b[38;5;241m=\u001b[39m summarize(book_reviews)\n\u001b[1;32m     23\u001b[0m     final_book_summary \u001b[38;5;241m=\u001b[39m summarize_final(book_summaries)\n\u001b[0;32m---> 25\u001b[0m     book_description \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_book_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_book_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     book_descriptions_dict[book] \u001b[38;5;241m=\u001b[39m book_description\n\u001b[1;32m     29\u001b[0m total_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m, in \u001b[0;36mgenerate_book_description\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m      5\u001b[0m description_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert the following summary into a structured book description that describes the book\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthemes, and significance: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m summary\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdescription_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m book_description \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(book_description)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:196\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    194\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 196\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2079\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2072\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2073\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2074\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2075\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2076\u001b[0m     )\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2079\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2092\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2093\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2100\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3257\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:1642\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1638\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1639\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1640\u001b[0m         )\n\u001b[0;32m-> 1642\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1661\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:1528\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1521\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1522\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1523\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1524\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1525\u001b[0m     )\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1528\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:1380\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1368\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1369\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         use_cache,\n\u001b[1;32m   1378\u001b[0m     )\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1393\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:685\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    684\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    694\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:518\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 518\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start timer for the whole process\n",
    "total_start_time = time.time()\n",
    "\n",
    "unique_books = reviews['Book Title'].unique()\n",
    "random_books = random.sample(list(unique_books), 20)\n",
    "book_descriptions_dict = {}\n",
    "\n",
    "for book in random_books:\n",
    "    print(book)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for book in random_books:\n",
    "\n",
    "    book_subset = reviews[reviews['Book Title'] == book]\n",
    "\n",
    "    book_reviews = book_subset['Review Text'].dropna().tolist()\n",
    "\n",
    "    if not book_reviews:\n",
    "        continue\n",
    "        \n",
    "    book_summaries = summarize(book_reviews)\n",
    "    final_book_summary = summarize_final(book_summaries)\n",
    "\n",
    "    book_description = generate_book_description(final_book_summary)\n",
    "\n",
    "    book_descriptions_dict[book] = book_description\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"\\nTotal processing time: {total_duration:.2f} seconds\")\n",
    "\n",
    "for book, description in book_descriptions_dict.items():\n",
    "    print(\"\\n\")\n",
    "    print(f\"Book Title:\\n{book}\")\n",
    "    print(f\"Generated Description:\\n{description}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002899f3-9362-4d37-974e-ca577a06d2e5",
   "metadata": {},
   "source": [
    "---\n",
    "This process (obviously) took quite a while. To remedy this, we can make use of cloud computing in the form of Google Colab. Using their T4 GPU, it only took 2 minutes (almost 10x faster) to generate the following for another 20 random books:\n",
    "\n",
    "Total processing time: **141.88 seconds**\n",
    "\n",
    "**Book Title:**<br>\n",
    "Zero to One: Notes on Startups, or How to Build the Future<br>\n",
    "**Generated Description:**<br>\n",
    "Peter Thiel is a massive contrarian (and I'm against extreme contrarianism and extreme conformity), but his ideas are grounded and logical. He claims, for example, that nothing but information technology and communications technology have evolved much since the 70s. Instead of trying to find a niche in a monopolistic market, startups are supposed to find entirely different ideas. He also provides a number of other areas of thought for the entrepreneur when starting and running a business. I wouldn't generalize Thiel's wisdom to fields outside of startups (just like the case with Paul Graham) -- indeed he made some claims that were not well thought out.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Autobiography of Malcolm X<br>\n",
    "**Generated Description:**<br>\n",
    "Malcolm X's life was a struggle. May Allah forgive all his shortcomings and allow us to spread Islam with even a morsel or his enthusiasm, determination and commitment. It is a long and powerful story that reveals another facet of the fight for racial justice and equality. My copy of this book is already in tatters for the work I have done on it but I will buy a new one, frame it perhaps in a glass ceiling, and look at it day and night just to remind myself what a dedicated man can become but most importantly what he can overcome.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Bonhoeffer: Pastor, Martyr, Prophet, Spy<br>\n",
    "**Generated Description:**<br>\n",
    "The author Eric Metaxas must have spent years researching for this book. Bonhoeffer is an example of a man who loved the Word of God, the Church, and the person of Christ. His ideas/ theology. Was 3-stars, now 2-Stars Comment 20 September, 2023: I first read this book sometime in 2010-2011 - maybe prior to joining Goodreads, but certainly before I commenced writing any reviews. The book is so well-written and compelling. A fascinating life.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Surprised by Joy: The Shape of My Early Life<br>\n",
    "**Generated Description:**<br>\n",
    "\"I was equally angry with Him for creating a world.\" A young man who wishes to remain a sound Atheist cannot be too careful of his reading. A fairly dull middle, sandwiched between an interesting start and an interesting end. The book deepened my understanding of and appreciation for Lewis as a person, scholar, and author (not to mention adding new layers and depth of his other books). Reading how his life effected what he thought and wrote about is truly interesting. But I really loved this book.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Agony and the Ecstasy<br>\n",
    "**Generated Description:**<br>\n",
    "Author Irving Stone is a fantastic writer. I found myself reading slower and slower towards the end, because I did not want to finish reading the book! I felt the author devoted enough time to each event in Michelangelo's life to give it meaning and purpose, but was sure to move on when it was time. By the end of the book, you will feel that you know Michelangelo: family issues, rivalries, the popes, his friends, and his delight and obsession for his art. It also gives a excellent account of the the history and life in Italy in the 16th century.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Color and Light: A Guide for the Realist Painter (Volume 2)<br>\n",
    "**Generated Description:**<br>\n",
    "James Gurney's book is a clear and concise collection of information about the science of light color as they relate to observational (and imagined!) The lessons are observations of light and colors from James Gurny's years of painting experience. Convert the following summary into a structured book description that describes the book's content, themes, and significance: I learned more about color theory reading this book than I did studying illustration for two years at Pratt Institute. I will definitely be revisiting this book often!\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Art Thief: A True Story of Love, Crime, and a Dangerous Obsession<br>\n",
    "**Generated Description:**<br>\n",
    "This true story of a one of a kind criminal reads like fiction and I thoroughly enjoyed it. Police estimates he's stolen between $1 to 2 billion worth of art. He steals because he loves art. Obsession, madness, narcissism. He lived in an attic in his mother’s house with Anne-Catherine for most of his life where he accumulated his stolen art like a pack-rat. I could not stop listening to this audiobook! Nonfiction account of a prolific art heist.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Shoe Dog: A Memoir by the Creator of Nike<br>\n",
    "**Generated Description:**<br>\n",
    "The book is unexpectedly enjoyable to read - well written and intriguing - and is not your standard entrepreneurship story. The story of Phil Knight is everything books like \"Art of the Deal\" are not: the humble story of a dream, framed more as an accidental life journey than the story of rags to riches. He came off too proud, full of elitism and arrogance, he’s mysoginistic and extremely privileged. HOWEVER this book was pretty good and inspirational - message to work hard and believe in yourself.\n",
    "\n",
    "**Book Title:**<br>\n",
    "How to Win Friends & Influence People<br>\n",
    "**Generated Description:**<br>\n",
    "D.C. by Carnegie makes following his principles sound too easy. I think I expected a lot of dated advice since this was published so long ago, but I found myself enjoying this and also understanding why it remains so popular after so many decades. Some other things give me this weird feeling of 'fake superficiality' and it decribes a world where people can't disagree or be frank with eachother. But that's really just me trying to find something positive (using the \"principles\") in a book that I am still trying to UNlearn.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Anne of Green Gables<br>\n",
    "**Generated Description:**<br>\n",
    "This book made me cry. My daughter loved this book over the summer, one of the classics I’ve never read and it’s now firmly on my To Read list. I loved the writing so much and was surprised how lovely it is. Marilla and Rachel were great as well and I feel like Marilla had the most satisfying character development. This will be my comfort read, I'm obsessed and can't wait to watch the show Also Gilbert has my whole heart, so do all the other characters omg\n",
    "\n",
    "**Book Title:**\n",
    "The Practice of the Presence of God<br>\n",
    "**Generated Description:**<br>\n",
    "The life of Brother Lawrence is testimony to his writings; his single-minded concern for God, far from leading him away from love of people, brought him closer to them. The book is less practical than I was expecting, in terms of specific tips for spiritual disciplines. There were also times when his theology seemed a bit heavy on the idea that God sends all inflictions and suffering and sickness to purge/cure our soul. “We ought not to be weary of doing little things for the love of God, who regards not the greatness of the work, but with which it is performed”\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Horse and His Boy<br>\n",
    "**Generated Description:**<br>\n",
    "This is by far the best Narnia book. As a lover of redemption arcs, I find this story very satisfying. I love the relationship between Shasta and the talking horse, Bree. So many good scenes with Aslan. Several of his moments moved me almost to tears. The prince agrees, because he's so certain his evil plan will work. And yes, I love it. But a boy in battle is a danger only to his own side. I should have read this book sooner >.<\n",
    "\n",
    "**Book Title:**<br>\n",
    "Alexander Hamilton<br>\n",
    "**Generated Description:**<br>\n",
    "At over 800 pages, it’s a hefty read, and there were times when I felt bogged down by the sheer volume of detail. The author sometimes went so far in depth in creating supporting characters histories that I felt, at times, that I was reading someone else's biography. It's dense and has some passages that are somewhat dull, and even the succinct writing style of Chernow didn't ease it much. I would only recommend this to history buffs and someone researching Hamilton's life. Though it took 36 hours, it engaged my interest throughout.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Dreams from My Father: A Story of Race and Inheritance<br>\n",
    "**Generated Description:**<br>\n",
    "The book traces Obama's quest for self and purports that as a mixed race person with his family ties split between two continents and two cultures, he is confused and torn. I both understood and was puzzled by some of his feelings of loathing and anger towards himself and US society. I personally could connect with various aspects of his struggle and the larger struggle of the black community. The Kenya story is beautiful, but becomes a telling by his grandmother for 40 to 50 pages. I’m giving it 4 stars only because I felt as though the middle portion of the book dragged on a bit.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Color: A Natural History of the Palette<br>\n",
    "**Generated Description:**<br>\n",
    "Victoria Finlay is a very interesting person. She is lucky, too late for me to ever see them now, the extent of her journeys in the book is remarkable. I was expecting to read more of a history book, but it turned out to be a travelogue/memoir. The author is definitely part of the story, but for me, it lent a human aspect and interest that may otherwise have been lacking. Although a fan of micro-histories could certainly enjoy this book, they shouldn’t go in expecting a Simon Winchester style non fiction book.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Art Book<br>\n",
    "**Generated Description:**<br>\n",
    "This isn't an in-depth exploration. The pictures are of a very good quality and the book itself is a monster. Lots of diversity in terms of styles of art shown (not so much diversity in the artists) This is a wonderful book for art fans, as well as those who are art curious. Not for the academics, but that's part of the appeal. I used to flip through a copy of this and the companion Photography book at Superstore while my Mom shopped for groceries.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Intelligent Investor<br>\n",
    "**Generated Description:**<br>\n",
    "Graham is known for inspiring Warren Buffett, and many other major investors. He goes through different types of investors, starting from the defensive investor who is someone a lot more careful to speculate. Majority of stock buybacks are done so to counteract the execution of employee options. The key to value investing is purchasing stocks that are selling well below their “intrinsic” value. This is the best book on investing. A definitive read for those looking for a disciplined approach to investment. Many claim that it is still entirely relevant, despite its age.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Deep Work: Rules for Focused Success in a Distracted World<br>\n",
    "**Generated Description:**<br>\n",
    "\"Deep Work\" is about why and how to manage to work deeply, i.e., producing precious value. To learn quickly, you need to study for long periods of time consistently. Social media, if used without limit, can be particularly devastating to your quest to work deeper. To conclude, I’d like to recommend ‘Deep Work’ to anyone seeking to develop a more productive work routine. Read David Allen instead, whose ideas permeate this book to a degree, but who cannot be quoted every second page.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Mere Christianity<br>\n",
    "**Generated Description:**<br>\n",
    "\"I truly loved this book and recommend to all Christians ... even people who aren’t Christians will get a great grasp of what Christianity is all about\" \"I was alight with curiosity. The concepts and thoughts he introduced were really interesting, because what Lewis does is he takes normal, everyday concepts for a Christian and examines them to the point that you completely understand where and how and why they are\" \"His style of explaining things in such a blunt, straightforward manner was so relatable at times\"\n",
    "\n",
    "---\n",
    "Generating summaries from book reviews is a powerful approach to capturing the essence of a book through the perspectives of multiple readers. By condensing a range of opinions, experiences, and highlights from reviews, we can create a more balanced and comprehensive description that goes beyond a typical synopsis. This method of summary generation can be applied to many other forms of reviews (product, location, etc) and has been done by companies like Amazon for their product reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ff5c4-16d0-44db-9776-32baab367ad5",
   "metadata": {},
   "source": [
    "# Part 2: Manual Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8461ae-172c-4c6f-9be0-4550e3e35c53",
   "metadata": {},
   "source": [
    "## Star Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d7bf3-a606-4725-8df9-5e379a45aea7",
   "metadata": {},
   "source": [
    "Users give a book a _Star Rating_ along with leaving a _text review_ on a book, we investigate the question: \"Can a Neural Network be used to accurately predict the number of stars of a review based on the text?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48a0b09d-a8f7-45cc-8080-d0a56cff5b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Review Text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b91c61-04e5-4d96-9f4e-be0300e6290c",
   "metadata": {},
   "source": [
    "Some reviews texts are empty, it is quite few. Therefore we can safely omit these reviews from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89bdd511-af11-4fa8-b282-a282a2174033",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_noNA = reviews.dropna(subset=['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87dca2ba-3bc6-4474-ad56-756e004e41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target and predictor datasets\n",
    "X = reviews_noNA['Review Text'].values\n",
    "y = reviews_noNA['Review Stars'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d2077-2278-458b-bb60-d61579b87348",
   "metadata": {},
   "source": [
    "We now use the `TfidfVectorizer` from `sklearn` to vectorize the text into term-frequency inverse-document frequency matrix form. This approach helps capture the importance of words across the dataset, where terms that occur frequently in a document but rarely across all documents are given higher weight, while common terms across the corpus are downweighted. The `LabelEncoder` converts categorical labels into numerical ones to feed into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6cab88f-a862-4d3c-bb2a-99d4d99592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75daf232-eaf2-4b1f-8b12-c901fe69cc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-10-23 15:09:13.823100: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5062 - loss: 1.2043 - val_accuracy: 0.5957 - val_loss: 0.9704\n",
      "Epoch 2/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7143 - loss: 0.7215 - val_accuracy: 0.6057 - val_loss: 0.9852\n",
      "Epoch 3/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8734 - loss: 0.3675 - val_accuracy: 0.6073 - val_loss: 1.2772\n",
      "Epoch 4/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1090 - val_accuracy: 0.5933 - val_loss: 1.7723\n",
      "Epoch 5/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0303 - val_accuracy: 0.6055 - val_loss: 2.3453\n",
      "Epoch 6/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9974 - loss: 0.0121 - val_accuracy: 0.6049 - val_loss: 2.5758\n",
      "Epoch 7/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9989 - loss: 0.0051 - val_accuracy: 0.6093 - val_loss: 2.8646\n",
      "Epoch 8/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0061 - val_accuracy: 0.5898 - val_loss: 2.9097\n",
      "Epoch 9/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 0.6069 - val_loss: 3.0439\n",
      "Epoch 10/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9983 - loss: 0.0045 - val_accuracy: 0.5974 - val_loss: 3.2282\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') # Softmax for categorical classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbde494c-a1f9-4ca8-8df2-a38f9fe5e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.600\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4355d99-9925-4f18-9dd7-b687ee5706e7",
   "metadata": {},
   "source": [
    "A test accuracy around 0.5 means we could have predicted 2.5 stars for every review and gotten the same accuracy. The training accuracy seems to increase drastically however validation accuracy remains steady at around 0.51. This indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc54f9-ee3b-409e-82c2-2c1704330955",
   "metadata": {},
   "source": [
    "What appears to be interesting is that despite the model statistically indicating overfitting, the predictions it makes can actually be quite reliable.\n",
    " Consider an example of a bad review. \"This book was absolutely terrible! How could you think this was a good idea.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82d55bf8-6ee0-4b07-bcb3-cc4db89a9714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted rating: 1\n"
     ]
    }
   ],
   "source": [
    "# BAD review example\n",
    "sample_review = [\"This book was absolutely terrible! How could you think this was a good idea.\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf4f2e-9ae0-4953-856e-07ac75750833",
   "metadata": {},
   "source": [
    "Consider an example of a long mixed review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2ec3016-8439-414d-a1e6-7d3083457bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Predicted rating: 3\n"
     ]
    }
   ],
   "source": [
    "sample_review = [\"I had high hopes for The Infinite Horizon after hearing so much about it. From the beginning, the premise seemed promising, and for the most part, it delivers on its intriguing concept. The plot revolves around a futuristic world where society grapples with the boundaries of artificial intelligence, humanity, and survival—concepts that have always fascinated me. The world-building is impressive, with detailed landscapes and a unique societal structure that keeps you hooked initially. The author has clearly put a lot of thought into constructing the futuristic world, and it shows in the vivid descriptions and creative technologies. However, while the world-building is rich, the characters left much to be desired. The protagonist, Lila, felt underdeveloped. I found myself frustrated at several points because her motivations were either unclear or inconsistent. In the beginning, she starts off as a strong, determined character, but midway through, her actions seem erratic and her growth stagnates. The dialogue, too, felt stilted at times, making it hard to connect with the characters emotionally. There were a few moments where I felt the conversations between key characters were forced, almost like they were inserted to explain plot points rather than feeling organic.On the flip side, I have to give credit where it’s due—the pacing of the story is solid for the most part. There are intense moments where you’re on the edge of your seat, particularly during the battle scenes. These scenes were written with such vivid detail that I could easily imagine them playing out in a movie. The action sequences are well thought out, and they definitely add excitement to the narrative. That being said, there were also moments where the pacing lagged, especially in the middle sections. Some chapters felt like filler, dragging on with unnecessary exposition and side plots that didn’t add much to the overarching story.\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a201d-8fa7-4364-9b07-583dfc480d63",
   "metadata": {},
   "source": [
    "The rating prediction appears to be somewhat reliable even with longer mixed reviews. Now let's consider an example of a great review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8b1445f-7652-4a9f-8d53-cb3f6179ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Predicted rating: 5\n"
     ]
    }
   ],
   "source": [
    "sample_review = [\"This book was a captivating read from start to finish. The characters felt incredibly real, and the plot twists kept me on the edge of my seat. I couldn't put it down and will definitely be recommending it to everyone!\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdca806-235b-4aa0-ab0b-27422cc4c9e7",
   "metadata": {},
   "source": [
    "#### Alternative models to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e336c8e2-7766-4c4b-853b-c7834e1d4d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.4955 - loss: 1.2645 - val_accuracy: 0.5750 - val_loss: 1.0008\n",
      "Epoch 2/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6486 - loss: 0.8598 - val_accuracy: 0.6035 - val_loss: 0.9826\n",
      "Epoch 3/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.6207 - val_accuracy: 0.5980 - val_loss: 1.0843\n",
      "Epoch 4/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8579 - loss: 0.4033 - val_accuracy: 0.5974 - val_loss: 1.2660\n",
      "Epoch 5/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9076 - loss: 0.2683 - val_accuracy: 0.5867 - val_loss: 1.5624\n",
      "Epoch 6/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9370 - loss: 0.1895 - val_accuracy: 0.5961 - val_loss: 1.8801\n",
      "Epoch 7/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1408 - val_accuracy: 0.5946 - val_loss: 2.1300\n",
      "Epoch 8/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9622 - loss: 0.1162 - val_accuracy: 0.6073 - val_loss: 2.2450\n",
      "Epoch 9/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9655 - loss: 0.1072 - val_accuracy: 0.5944 - val_loss: 2.3953\n",
      "Epoch 10/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.0895 - val_accuracy: 0.5895 - val_loss: 2.6140\n"
     ]
    }
   ],
   "source": [
    "# Alternative model incorporating Dropout\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "372999e7-4c7a-48ec-948a-7154e044fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042c812-4f84-464a-a53d-e0ca3381c91d",
   "metadata": {},
   "source": [
    "Adding dropout layers does not improve the statistical overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1524b67-e90e-424c-8cd1-0a6c7621cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use L2 Regularization to attempt to improve overfitting through weight decay\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81520d4f-1862-4d54-b3c8-f1772d04b429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4922 - loss: 1.8909 - val_accuracy: 0.4945 - val_loss: 1.2885\n",
      "Epoch 2/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5009 - loss: 1.2810 - val_accuracy: 0.4945 - val_loss: 1.2767\n",
      "Epoch 3/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4983 - loss: 1.2770 - val_accuracy: 0.4945 - val_loss: 1.2739\n",
      "Epoch 4/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4962 - loss: 1.2663 - val_accuracy: 0.4945 - val_loss: 1.2718\n",
      "Epoch 5/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4981 - loss: 1.2690 - val_accuracy: 0.4945 - val_loss: 1.2705\n",
      "Epoch 6/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4962 - loss: 1.2649 - val_accuracy: 0.4945 - val_loss: 1.2701\n",
      "Epoch 7/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4921 - loss: 1.2759 - val_accuracy: 0.4945 - val_loss: 1.2697\n",
      "Epoch 8/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4950 - loss: 1.2708 - val_accuracy: 0.4945 - val_loss: 1.2694\n",
      "Epoch 9/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4992 - loss: 1.2750 - val_accuracy: 0.4945 - val_loss: 1.2693\n",
      "Epoch 10/10\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4922 - loss: 1.2782 - val_accuracy: 0.4945 - val_loss: 1.2696\n",
      "Test accuracy: 0.507\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1530fdb-f3cc-4a36-ba60-70b7dd828616",
   "metadata": {},
   "source": [
    "When applying L2 regularization the result is the model tends to _underfit_. We see this because both the training and validation accuracies are low and nearly identical, meaning the model is not complex enough to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455b952-4e93-4212-9263-c1fc2f1c1b82",
   "metadata": {},
   "source": [
    "# Part 3: Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11b440-d094-4065-81ba-62d94fccef35",
   "metadata": {},
   "source": [
    "Books are often reviewed based on certain aspects, for example, a fiction book on the plot, characters or maybe the setting. Readers have diverse tastes and some may prioritise plot over characters or maybe emotion over humour. We therefore propose an Aspect-Based Sentiment Analysis (ABSA) of our book reviews. This can help identify strengths and can guide authors and publishers towards understanding the reader's perspective. By highlighting both positive and negative aspects, we can produce a balanced critique with a well-rounded perspective. This also enhances decision-making for readers who do not want to sift through hundreds of wordy reviews to gauge a sentiment on a specific aspect of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd156b3a-112e-48fb-b6f2-78b58f5066e8",
   "metadata": {},
   "source": [
    "## [3A] Simple Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30554d-64fa-466d-a497-22ded57f8313",
   "metadata": {},
   "source": [
    "A simple and interpretable approach to this problem is to:\n",
    "1. Pre-define certain aspects.\n",
    "2. Select sentences containing that aspect.\n",
    "3. Compute sentiment of those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fab6abc-d846-483c-b4c7-f9671ddf27cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Define aspects we're interested in\n",
    "aspects = [\"plot\", \"characters\", \"writing\", \"pacing\", \"setting\", \"structure\", \"emotion\", \"humor\"]\n",
    "\n",
    "def preprocess_lemmatize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def extract_aspects(tokens):\n",
    "    extracted_aspects = []\n",
    "    for aspect in aspects:\n",
    "        if aspect in tokens:\n",
    "            extracted_aspects.append(aspect)\n",
    "    return extracted_aspects\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def extract_aspect_sentences(text, aspect):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [sent for sent in sentences if aspect in sent.lower()]\n",
    "\n",
    "def analyze_review(review):\n",
    "    tokens = preprocess_lemmatize(review)\n",
    "    extracted_aspects = extract_aspects(tokens)\n",
    "    results = {}\n",
    "    \n",
    "    for aspect in extracted_aspects:\n",
    "        aspect_sentences = extract_aspect_sentences(review, aspect)\n",
    "        if aspect_sentences:\n",
    "            sentiment = sum(analyze_sentiment(sent) for sent in aspect_sentences) / len(aspect_sentences)\n",
    "            results[aspect] = {\n",
    "                \"sentiment\": sentiment,\n",
    "                \"sentences\": aspect_sentences\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eff70144-c503-4fc0-a518-8fa3d643a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: plot\n",
      "Sentiment: 0.30\n",
      "Supporting sentences:\n",
      "- \n",
      "The book had an intriguing plot that kept me guessing until the end.\n",
      "\n",
      "Aspect: writing\n",
      "Sentiment: 0.12\n",
      "Supporting sentences:\n",
      "- The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
      "\n",
      "Aspect: pacing\n",
      "Sentiment: -0.15\n",
      "Supporting sentences:\n",
      "- However, the pacing was a bit slow in the middle sections.\n",
      "\n",
      "Aspect: setting\n",
      "Sentiment: 0.12\n",
      "Supporting sentences:\n",
      "- The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best case example\n",
    "example_review = \"\"\"\n",
    "The book had an intriguing plot that kept me guessing until the end. \n",
    "The characters were well-developed and relatable. \n",
    "However, the pacing was a bit slow in the middle sections. \n",
    "The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
    "\"\"\"\n",
    "\n",
    "results = analyze_review(example_review)\n",
    "\n",
    "for aspect, data in results.items():\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Sentiment: {data['sentiment']:.2f}\")\n",
    "    print(\"Supporting sentences:\")\n",
    "    for sentence in data['sentences']:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd4f6693-431f-4817-a814-34bee5dceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the correct subset\n",
    "born_a_crime_subset = reviews[reviews['Book Title'] == 'Born a Crime: Stories From a South African Childhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68837edd-0b94-40ec-b1f1-04e0c3e203a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'d rate this 4.5 stars. I was really surprised when Trevor Noah was named Jon Stewart\\'s successor on The Daily Show . I inherently knew that they wouldn\\'t pick someone with a sense of humor and style identical to Stewart\\'s, but I felt that Noah was so different that his selection meant the show would have a really different feel, which might not appeal to long-time fans of the show. But I always root for the underdog, so as he was getting savaged by critics and fans in his first few days on the job, I kept hoping he\\'d be able to tough it out and show the stuff—comedic and otherwise—of which he was made. After reading , I realize that I needn\\'t have worried about Trevor Noah. For a child growing up in South Africa in the last days of, and the tumult following apartheid, he faced crises far greater than dissatisfied fans. And if he could be raised during such a crazily illogical time in a country where more violence, racism, and mistreatment went unreported than caught the media\\'s eye, he\\'d have no problem skewering the insanity of our political system, especially leading into the election of 2016!! \"On February 20, 1984, my mother checked into Hillbrow Hospital for a scheduled C-section delivery. Estranged from her family, pregnant by a man she could not be seen with in public, she was alone. The doctors took her up to the delivery room, cut open her belly, and reached in and pulled out a half-white, half-black child who violated any number of laws, statutes, and regulations—I was born a crime.\" Born to a black Xhosa mother and a white Swiss father, Noah literally spent his earliest days hiding indoors. His parents, who never married, couldn\\'t be seen together, and because his mother looked so different than he did, she couldn\\'t walk through the streets with him, because at any moment someone might accuse her of kidnapping another person\\'s child. Yet while their lives dealt with crushing poverty, violence, and racism from all sides, his deeply religious mother never let anything bother her, or stop her from raising her son to know he was loved, and to know that he truly could accomplish anything he wanted, despite all of the obstacles in his way. \"She taught me to challenge authority and question the system. The only way it backfired on her was that I constantly challenged and questioned her.\" provides a first-hand account of the last days of apartheid and its aftermath, and what it was like to grow up as a mixed-race child, where he wasn\\'t white enough to be considered white, nor was he black enough to be considered black. While at times this had its advantages, for the most part, it left him on the outside looking in, having to handle everything on his own, fight his own battles, struggle to find people who genuinely liked him for who he was and not the novelty of his skin color, and rebel against a mother who only wanted him to behave. If you go into this book expecting to laugh hysterically because of Noah\\'s day job, think again. While the book does include some of the wry humor that has begun endearing him to fans, this is an emotional, brutal, and educational story of a life which flourished despite the odds stacked against it. This is a book about growing up in a culture of poverty and crime, and how easy it was to get caught up in that, especially when it was one of the only ways to make money and be able to feed, clothe, and enjoy yourself. It\\'s also a book about fear, how it motivates you, how it paralyzes you, and how it threatens to take away the one thing you cherish more than any other. More than anything, though, this is a book about the unwavering love of a mother for a child she chose to have. She knew it would be difficult raising her son in the age of apartheid, and in fact, she had no idea when he was born that it would end anytime soon. But Noah was a remarkable child, and while he exasperated, frightened, and upset his mother from time to time, she knew he would accomplish great things one day (as soon as he stopped putting cornrows in his hair and hanging out with those awful hoodlums he called friends). I enjoyed this book and learned a lot about apartheid, which I really didn\\'t know much about. Noah is a good writer, and delivered his narrative much as I\\'ve heard him deliver his lines on . This is a funny, thought-provoking, and emotional book, although I felt that some of his anecdotes went on a little too long, while others didn\\'t go on long enough. I also would have liked to have learned how he went from his upbringing in South Africa to one day hosting an acclaimed television show—other than passing mentions of things he did, I have no idea how he made the leap. I\\'ve heard some people say that the audio version of this book is brilliant because Noah reads it himself, but if you read the print/digital version, you can still hear his voice through his words. Noah\\'s story is a lesson of the inequities of the past, and a warning for what is still possible to happen again in our world. But this isn\\'t heavy-handed; it\\'s fun, insightful, and very compelling. See all of my reviews at , and see my list of the best books I read in 2016 at .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_subset['Review Text'][7679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5fcd468-0e81-4882-b9ed-f24158bad81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: humor\n",
      "Sentiment: -0.02\n",
      "Supporting sentences:\n",
      "- I inherently knew that they wouldn't pick someone with a sense of humor and style identical to Stewart's, but I felt that Noah was so different that his selection meant the show would have a really different feel, which might not appeal to long-time fans of the show.\n",
      "- While the book does include some of the wry humor that has begun endearing him to fans, this is an emotional, brutal, and educational story of a life which flourished despite the odds stacked against it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "born_a_crime_subset['Review Text'][7679]\n",
    "results = analyze_review(born_a_crime_subset['Review Text'][7679])\n",
    "\n",
    "for aspect, data in results.items():\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Sentiment: {data['sentiment']:.2f}\")\n",
    "    print(\"Supporting sentences:\")\n",
    "    for sentence in data['sentences']:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c7ff1-6bef-4587-b255-9014548db923",
   "metadata": {},
   "source": [
    "## [3B] Advanced Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95a440c4-0a45-44b1-90f3-827f02720386",
   "metadata": {},
   "source": [
    "The ABSA field is an extremely active area of research. A popular dataset which is often used as a benchmark for ABSA models is the [SemEval 2014 Task 4](https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval). We considered many alternative papers and datasets, and finally settled on [InstructABSA](https://arxiv.org/abs/2302.08624v6) due to its incredible performance on benchmarks and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73220cb-4421-48bd-999b-383c254d8638",
   "metadata": {},
   "source": [
    "### InstructABSA Architecture\n",
    "This model introduces positive, negative, and neutral examples to each training sample, and instruction tune the model [Tk-Instruct](https://aclanthology.org/2022.emnlp-main.340/) for ABSA subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b57336-dffa-4492-a842-0fcecfaff55f",
   "metadata": {},
   "source": [
    "The ABSA subtasks can be represented as follows: Let $S_i$ represent the $i^{th}$ review sentence in the training sample, where $S_i = {w_{i}^1, w_{i}^2, ..., w_{i}^n}$ with $n$ as the number of tokens in the sentence. \n",
    "Each $S_i$ contains a set of aspect terms denoted by $A_i = {a_{i}^1, a_{i}^2, ..., a_{i}^m} | m \\le n$, and the corresponding opinion terms and sentiment polarities for each aspect term are denoted by $O_{i} = {o_{i}^1, o_{i}^2, ..., o_{i}^m}$ and $SP_{i} = {sp_{i}^1, sp_{i}^2, ..., sp_{i}^m}$ respectively, where $sp_i^k \\in [ positive, negative, neutral ]$. \n",
    "\\\n",
    "\\\n",
    "The ABSA tasks are described as follows:\\\n",
    "ATE: $A_i = LM_{ATE}(S_i)$\\\n",
    "ATSC: $sp_i^k = LM_{ATSC}(S_i, a_i^k)$\\\n",
    "ASPE: $[A_i, SP_i] = LM_{ASPE}(S_i)$\\\n",
    "AOOE: $o_{i}^k = LM_{AOOE}(S_i, a_i^k)$\\\n",
    "AOPE: $[A_i, O_i] = LM_{AOPE}(S_i)$\\\n",
    "AOSTE: $[A_i, O_i, SP_i] = LM_{AOSTE}(S_i)$\\\n",
    "\\\n",
    "In these equations, $LM$ represents the language model, and the corresponding inputs and outputs are defined accordingly. InstructABSA, in their approach, instruction tuned $LM_{subtask}$ by prepending task-specific prompts to each input sample to arrive at $LM_{subtask}^{Inst}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8f9ae-1d28-4279-b8b6-f47dac9c4757",
   "metadata": {},
   "source": [
    "We specifically consider ATSC for our use case of identify the sentiment around a specific aspect of a book review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d6135-78f3-46d0-a3b6-71ea7a1800f4",
   "metadata": {},
   "source": [
    "![ABSASubtasks.png](images/ABSASubtasks.png)\n",
    "\n",
    "Image Source: [InstructABSA](https://arxiv.org/abs/2302.08624v6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6ee53-d216-4cc6-a5a2-94bcc377c524",
   "metadata": {},
   "source": [
    "![overview.png](images/overview.png)\n",
    "Image Source: [InstructABSA](https://arxiv.org/abs/2302.08624v6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a94eb1-b978-4334-956e-56a35ef6a8ab",
   "metadata": {},
   "source": [
    "### Tk-INSTRUCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99663bd6-285d-461a-a9c1-280e44dd3ee6",
   "metadata": {},
   "source": [
    "Tk-INSTRUCT is a transformer model trained to follow a variety of in-context instructions (plain language task definitions or _k_-shot examples). It builds on the T5 text-to-text transformer model using an instruction tuning approach. It converts diverse NLP tasks into a consistent instruction format through a task format:\n",
    "\n",
    "- **Definition:** Task description\n",
    "- **Things to avoid:** Common mistakes\n",
    "- **Positive examples:** Good completions\n",
    "- **Negative examples:** Poor completions\n",
    "- **Input** \n",
    "- **Output**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eea556-1039-49a8-94a1-8cee57995750",
   "metadata": {},
   "source": [
    "### Understanding the T5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0436d-2381-4f66-9b0f-f26b1f61225e",
   "metadata": {},
   "source": [
    "![ABSA.png](images/ABSA.png)\n",
    "Part of this image was adapted from: [Jay Alammar’s blog](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5f437-4dcd-49e1-bf74-7cf0af1673fd",
   "metadata": {},
   "source": [
    "At the bottom of the hierarchy of what we use lies T5, which stands for  _Text-To-Text Transfer Transformer_ proposed by [Google in 2020](https://arxiv.org/abs/1910.10683). It was trained on a cleaned common crawl web extracted [text corpus](https://www.tensorflow.org/datasets/catalog/c4). It is based on a BERT-sized encoder-decoder transformer which is illustrated in the image above. Since the dataset is unlabelled, an unsupervised objective was selected to allow learning from the unlabelled data. Words are dropped out independently and uniformly at random and replaced with a unique sentinel token. The model is then trained to predict sentinal tokens to delineate the dropped-out text (refer to image below). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b98eaa-f0f1-4fa3-bec9-22b955e95dcb",
   "metadata": {},
   "source": [
    "![T5objective.png](images/T5objective.png)\n",
    "Image Source: [T5 Paper](https://arxiv.org/abs/1910.10683) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3037021-c663-45ee-921c-d4858af5ec9a",
   "metadata": {},
   "source": [
    "To focus in on the core structure of the T5 transformer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f86343-0908-4aa2-8cc1-05a53e048b69",
   "metadata": {},
   "source": [
    "**Input Representation**\n",
    "\n",
    "1. Tokenization:\n",
    "\n",
    "    Uses `SentencePiece`, which creates a vocabulary of subword units from an input text. For example the instruction \"Summarize this review: This book rules!\" could be tokenized into [\"summarize\", \"_this\", \"_review\", \":\", \"_This\", \"_book\", \"_rules\", \"!\"].\n",
    "\n",
    "3. Conversion to Token IDs:\n",
    "\n",
    "    Every token is mapped to a unique integer ID from the vocabulary.\n",
    "\n",
    "4. Embedding:\n",
    "\n",
    "   The token IDs are then converted into dense vector embeddings, along with positional embeddings to encode the token's position in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eceee0e-657a-4d9a-a6bd-3d3a1f9d37eb",
   "metadata": {},
   "source": [
    "\n",
    "**Encoder**\n",
    "\n",
    "The encoder stage is built with multiple layers (12 layers in T5-Base, the smallest T5 model). Each layer consists of:\n",
    "\n",
    "1. Self-Attention Mechanism:\n",
    "\n",
    "   This transforms the input text into 3 vectors through linear transformations of the input: _query_ ($Q$), _key_ ($K$) and _value_ ($V$). The attention mechanism then calculates a weighted sum of generated values based on the similarity between query and key vectors. It essentially takes into consideration the relationship among words within the same sentence. With self-attention, we are feeding the **same embedding into all 3 layers**. The attention formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08529e-a187-4278-b1a5-0b71efd85890",
   "metadata": {},
   "source": [
    "![SelfAttention.png](images/SelfAttention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5892de-2377-498c-b00f-2715df5404ba",
   "metadata": {},
   "source": [
    "3. Feed-Forward Neural Network:\n",
    "\n",
    "    2-layered fully connected network using the ReLU activation.\n",
    "\n",
    "4. Layer Normalization:\n",
    "\n",
    "    Applied before each sub-layer to stabilize training (pre-norm), this makes the model more robust to learning rate changes. T5 does not use scaling (gamma) and bias (beta) parameters.\n",
    "\n",
    "5. Residual Connections:\n",
    "\n",
    "   Skip-connections are also included around each sub-layer to improve gradient flow during training. This ensures the model does not rely on certain weights too heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a549a0-d737-4bcc-a397-c359fe685b79",
   "metadata": {},
   "source": [
    "**Decoder**\n",
    "\n",
    "The decoder also consists of multiple layers (12 layers in T5-Base). Each layer includes:\n",
    "\n",
    "1. Self-Attention Mechanism:\n",
    " \n",
    "    The implementation is similar to the encoder. The addition of masking ensures that positions do not attend to future positions in the output sequence.\n",
    "\n",
    "3. Cross-Attention Mechanism (Encoder-Decoder Attention):\n",
    "\n",
    "    This enables each position in the decoder to attend to all positions in the encoder’s output.\n",
    "\n",
    "4. Feed-Forward Neural Network:\n",
    "\n",
    "    Same concept as the encoder.\n",
    "\n",
    "5. Layer Normalization:\n",
    "\n",
    "    Applied before each sub-layer (pre-norm), similarly to the encoder.\n",
    "\n",
    "6. Residual Connections:\n",
    "\n",
    "    Same as the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714269b4-45c5-4e91-9de1-ce6e3e97fe61",
   "metadata": {},
   "source": [
    "### Applying InstructABSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae55e1c-d8a2-4469-b5b9-a25659a48cd5",
   "metadata": {},
   "source": [
    "We begin by preloading InstructABSA from [HuggingFace](https://huggingface.co/kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined). With that we predefine certain prompts for the aspect-based sentiment analysis. This primes InstructABSA for our task. We continue to pass in the reviews from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f47cba7e-29d7-4ff5-9b57-da9d421940c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c7c8d5e-81ae-462a-ab49-4220ef8384f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [\"plot\", \"writing\", \"pacing\", \"setting\", \"structure\", \"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0d88906-9962-463b-a084-ac6434c5ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined\")\n",
    "\n",
    "# Predefine prompt for aspect based sentiment analysis\n",
    "bos_instruction = \"\"\"Definition: The output will be 'positive' if the aspect identified in the sentence contains a positive sentiment. If the sentiment of the identified aspect in the input is negative, the answer will be 'negative'. Otherwise, the output should be 'neutral'. For aspects which are classified as noaspectterm, the sentiment is none.\n",
    "Positive example:\n",
    "input: The story is masterfully crafted, with unexpected twists that kept me engrossed until the final page. The aspect is plot.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The plot becomes increasingly nonsensical, with too many coincidences to be believable. The aspect is plot.\n",
    "output: negative\n",
    "Positive example:\n",
    "input: The prose is elegant and precise, with vivid descriptions that bring each scene to life. The aspect is writing.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The writing is clunky and amateurish, full of clichés and redundant phrases. The aspect is writing.\n",
    "output: negative\n",
    "Positive example:\n",
    "input: The story moves at a perfect rhythm, building tension steadily while giving readers time to process key moments. The aspect is pacing.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The middle section drags endlessly, losing momentum and reader interest. The aspect is pacing.\n",
    "output: negative\n",
    "Neutral example:\n",
    "input: The book alternates between present-day scenes and flashbacks. The aspect is pacing.\n",
    "output: neutral\n",
    "Positive example:\n",
    "input: The dystopian world is brilliantly realized, with rich details that make it feel authentic and lived-in. The aspect is setting.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The setting lacks consistency and historical accuracy, breaking immersion repeatedly. The aspect is setting.\n",
    "output: negative\n",
    "Positive example:\n",
    "input: The innovative chapter arrangement enhances the mystery, revealing crucial information at perfect moments. The aspect is structure.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The disjointed narrative structure makes the story unnecessarily difficult to follow. The aspect is structure.\n",
    "output: negative\n",
    "Positive example:\n",
    "input: The emotional depth of the story is profound, creating genuine connections with the characters' struggles. The aspect is emotion.\n",
    "output: positive\n",
    "Negative example:\n",
    "input: The emotional scenes feel manipulative and artificial, failing to evoke any real feelings. The aspect is emotion.\n",
    "output: negative\"\"\"\n",
    "delim_instruct = ' The aspect is '\n",
    "eos_instruct = '.\\noutput:'\n",
    "\n",
    "def book_absa(book_title):\n",
    "    book_subset = reviews[reviews['Book Title'] == book_title]\n",
    "    book_reviews = book_subset['Review Text'].tolist()\n",
    "    book_summaries = summarize(book_reviews)\n",
    "    \n",
    "    book_final_summary = summarize_final(book_summaries)\n",
    "    \n",
    "    text = f'''{book_final_summary}'''\n",
    "    aspect_sentiment_dict = {}\n",
    "    \n",
    "    for aspect_term in aspects:\n",
    "        tokenized_text = tokenizer(bos_instruction + text + delim_instruct + aspect_term + eos_instruct, return_tensors=\"pt\")\n",
    "        output = model.generate(tokenized_text.input_ids)\n",
    "        sentiment_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Store the aspect term and its corresponding sentiment in the dictionary\n",
    "        aspect_sentiment_dict[aspect_term] = sentiment_output\n",
    "        print(f'Model output for {aspect_term}: ', sentiment_output)\n",
    "    \n",
    "    return aspect_sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c352f",
   "metadata": {},
   "source": [
    "#### Print the ABSA for various books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a2df5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book with the highest average rating: Night (Average Rating: 4.762711864406779)\n"
     ]
    }
   ],
   "source": [
    "reviews['Review Stars'] = pd.to_numeric(reviews['Review Stars'], errors='coerce')\n",
    "\n",
    "# Group by 'Book Title' and calculate the average rating\n",
    "average_ratings = reviews.groupby('Book Title')['Review Stars'].mean()\n",
    "\n",
    "# Find the book with the highest average rating\n",
    "highest_rated_book = average_ratings.idxmax()\n",
    "highest_average_rating = average_ratings.max()\n",
    "\n",
    "print(f'Book with the highest average rating: {highest_rated_book} (Average Rating: {highest_average_rating})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a681239-9e86-4770-89c4-4296f248efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6589ea9214a47e7946a0436502336c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae60b3527f94408aabf447be47775eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Model output for plot:  negative\n",
      "Model output for writing:  negative\n",
      "Model output for pacing:  negative\n",
      "Model output for setting:  negative\n",
      "Model output for structure:  negative\n",
      "Model output for emotion:  positive\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('Night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6affbf8f-d63b-4fca-9d0d-40cafb8e4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plot': 'negative', 'writing': 'negative', 'pacing': 'negative', 'setting': 'negative', 'structure': 'negative', 'emotion': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print(absa_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "615f9cfa-946f-4915-b515-ba71785404ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39899ef5ca4d49c181ed4841bec11288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc4a7f0a43349eb852ab8658508d60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Model output for plot:  positive\n",
      "Model output for writing:  positive\n",
      "Model output for pacing:  positive\n",
      "Model output for setting:  positive\n",
      "Model output for structure:  positive\n",
      "Model output for emotion:  positive\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('Born a Crime: Stories From a South African Childhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "88319fe5-c98e-416a-85cc-25007d08496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 224\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14597704e1e44e783fcbe379e321306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba63373d3983407c867f57aa367625de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Model output for plot:  positive\n",
      "Model output for writing:  negative\n",
      "Model output for pacing:  positive\n",
      "Model output for setting:  positive\n",
      "Model output for structure:  positive\n",
      "Model output for emotion:  positive\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('The Art Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9cfa4b5-f3d4-4866-80ec-a4d18d61ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e13e365a6e541d59fd7a5bec494862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe869f6fb83d4ec8a86564c158cf001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Model output for plot:  neutral\n",
      "Model output for writing:  positive\n",
      "Model output for pacing:  neutral\n",
      "Model output for setting:  neutral\n",
      "Model output for structure:  neutral\n",
      "Model output for emotion:  neutral\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('Cleopatra: A Life')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0e33afe7-4d2d-4193-aae4-0ffa17decee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2711e9fae8974d698f572e3c020c6476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d77e23a7584753a7247c18a644ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Model output for plot:  positive\n",
      "Model output for writing:  negative\n",
      "Model output for pacing:  positive\n",
      "Model output for setting:  positive\n",
      "Model output for structure:  positive\n",
      "Model output for emotion:  positive\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('Charlie and the Chocolate Factory')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
