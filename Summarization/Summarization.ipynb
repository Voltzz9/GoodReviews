{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bab3fc-d83f-40fd-8ca4-8590f72b3114",
   "metadata": {},
   "source": [
    "# Data Science 346 Project Stellenbosch University\n",
    "### Team:\n",
    "- David Nicolay 26296918\n",
    "- Kellen Mossner 26024284\n",
    "- Matthew Holm 26067404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8157126f-431f-448c-94bb-4b8464043d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set Random seed for reproducible results\n",
    "random_seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a029d21-988a-4555-8819-97ffab1b54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\User-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\User-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\User-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK packages downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caea788-fb05-4751-87eb-22194b033b23",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281542c-d91f-41d8-83e9-93a9156093f7",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2703d11d-77df-4cac-be67-7281b193669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Review Stars</th>\n",
       "      <th>Review Likes</th>\n",
       "      <th>Genres</th>\n",
       "      <th>First Published Date</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>This book is based on a television series whic...</td>\n",
       "      <td>September 29, 2014</td>\n",
       "      <td>5</td>\n",
       "      <td>513</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>I am not the audience for this book, mainly be...</td>\n",
       "      <td>June 3, 2014</td>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>Way of Seeing, John Berger Ways of Seeing is a...</td>\n",
       "      <td>October 21, 2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>First of all, this entire book is set in bold....</td>\n",
       "      <td>May 25, 2008</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ways of Seeing</td>\n",
       "      <td>https://www.goodreads.com/book/show/2784.Ways_...</td>\n",
       "      <td>This was a great introduction to the work of J...</td>\n",
       "      <td>March 12, 2020</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>Art, Nonfiction, Philosophy, Essays, Art Histo...</td>\n",
       "      <td>January 1, 1972</td>\n",
       "      <td>John Berger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Book Title                                               Link  \\\n",
       "0  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "1  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "2  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "3  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "4  Ways of Seeing  https://www.goodreads.com/book/show/2784.Ways_...   \n",
       "\n",
       "                                         Review Text         Review Date  \\\n",
       "0  This book is based on a television series whic...  September 29, 2014   \n",
       "1  I am not the audience for this book, mainly be...        June 3, 2014   \n",
       "2  Way of Seeing, John Berger Ways of Seeing is a...    October 21, 2021   \n",
       "3  First of all, this entire book is set in bold....        May 25, 2008   \n",
       "4  This was a great introduction to the work of J...      March 12, 2020   \n",
       "\n",
       "   Review Stars  Review Likes  \\\n",
       "0             5           513   \n",
       "1             3           216   \n",
       "2             4             0   \n",
       "3             4           106   \n",
       "4             4            80   \n",
       "\n",
       "                                              Genres First Published Date  \\\n",
       "0  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "1  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "2  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "3  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "4  Art, Nonfiction, Philosophy, Essays, Art Histo...      January 1, 1972   \n",
       "\n",
       "        Author  \n",
       "0  John Berger  \n",
       "1  John Berger  \n",
       "2  John Berger  \n",
       "3  John Berger  \n",
       "4  John Berger  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "reviews = pd.read_csv(\"../WebScrapingExplore/data/goodreads_reviews_all.csv\")\n",
    "book_genres = pd.read_csv(\"../WebScrapingExplore/data/book_info.csv\")\n",
    "\n",
    "reviews = pd.merge(reviews, book_genres, on='Book Title', how='inner')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0e4d9244-2fe3-4501-8941-9e2d77736ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34619 entries, 0 to 35518\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Book Title            34619 non-null  object\n",
      " 1   Link                  34619 non-null  object\n",
      " 2   Review Text           34619 non-null  object\n",
      " 3   Review Date           34619 non-null  object\n",
      " 4   Review Stars          34619 non-null  int64 \n",
      " 5   Review Likes          34619 non-null  int64 \n",
      " 6   Genres                34619 non-null  object\n",
      " 7   First Published Date  34619 non-null  object\n",
      " 8   Author                34619 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb9d81-b081-4986-bcd9-a7a06bf58b86",
   "metadata": {},
   "source": [
    "Through investiagting our database further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0884873-89a2-40dc-bfc4-0372e2c0a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language\n",
    "def is_english(review):\n",
    "    try:\n",
    "        return detect(review) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0154a7e1-b5dd-47d8-be52-65d8a7316a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only English reviews- This may take a while\n",
    "reviews = reviews[reviews['Review Text'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92630c16-ffb5-47ee-8263-d366d0a4607f",
   "metadata": {},
   "source": [
    "# Part 1: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01b48e-9643-47bc-adde-ed87e63ef940",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9c880-0873-41fe-b259-8df4176ab9be",
   "metadata": {},
   "source": [
    "Initializing the pipeline will take a while to run at first, since this function downloads the model weights (about 1.6gb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fbd3b63-8eeb-4846-a70b-6e29e4dab7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ebbad676-5114-4c67-9fb7-147d0a612722",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf2f02-1f0c-4c0b-b779-528fb3497d0d",
   "metadata": {},
   "source": [
    "Due to restricted input length of the summarizer the reviews text needs to be divided into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "62176a75-3abc-4304-adc1-55410253691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    for word in words:\n",
    "        if current_size + len(word) > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_size = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_size += len(word) + 1  # +1 for space\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, max_summary_length=150):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=max_summary_length, min_length=10)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    final_summary = ' '.join(summaries)\n",
    "    if len(final_summary) > max_summary_length:\n",
    "        final_summary = summarizer(final_summary, max_length=max_summary_length, min_length=30)[0]['summary_text']\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6df7e-0359-45a4-9b50-40ff115e4650",
   "metadata": {},
   "source": [
    "Begin by summarizing 1 book's reviews - \"Ways of Seeing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba206aa4-bb28-4c99-9aa5-668b6f98e2ca",
   "metadata": {},
   "source": [
    "Here we can have a look at how the model does a good job of summarizing (but it essentially picks important sentences), however we still need to present it in a format that explains the overall sentiment from the readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43c2f252-4952-477d-8184-ea5ec09ca297",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test = summarizer(book_df.loc[3]['Review Text'], max_length=50, min_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19feaaa1-c1b0-4ce8-a7ef-459ae795c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '4 essays and 3 pictorial essays. It seems like museums are doing a lot of things wrong as well as right. Chapter on oil-painting was particularly interesting but it was the last one about advertising (or \"publicity\"'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6de8201a-f1e9-4586-af53-211c09f377c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First of all, this entire book is set in bold. I don\\'t know what crazy crazyman let that through the gate at Penguin but I just felt I had to point it out right away. It\\'s still worth reading. 4 essays and 3 pictorial essays. Really interesting stuff cutting away some of the bullshit associated with our appreciation of art. It seems like museums are doing a lot of things wrong as well as right. Chapter on oil-painting was particularly interesting but it was the last one about advertising (or \"publicity\" as it\\'s exclusively referred to in this book) that has me thinking. Advertising not only needs you to want this shirt, this car, the entire industry must endeavor to narrow the scope of your desires to make you amenable to the culture. The mindset must always be a future, better you achieved through important purchases. The essay is horrifying enough until you realise that it\\'s thirty years old, and this is now only one facet of a business that\\'s grown much more insidious. The ads shown are almost quaint in their straight sell.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df.loc[3]['Review Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb0cbf-70c2-4b49-815a-870eb2429542",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8eb12e3b-d37c-434f-a750-590ac14f3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************************\n",
    "def preprocess(reviews):\n",
    "    \"\"\"\n",
    "    Replaces newline characters with spaces\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    print(f\"Number of reviews: {n_reviews}\")\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        # Replace newlines with spaces\n",
    "        reviews[i] = review.replace('\\n', ' ').strip()\n",
    "        \n",
    "        \n",
    "def split_sentences(reviews):\n",
    "    \"\"\"\n",
    "    Splits the reviews into individual sentences.\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        # import nltk\n",
    "        # nltk.download('punkt')\n",
    "        sentences = sent_tokenize(review)  # Tokenize into sentences\n",
    "        # Remove empty sentences and strip spaces\n",
    "        sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        reviews[i] = sentences\n",
    "        \n",
    "        \n",
    "def encode_sentences(reviews):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the reviews\n",
    "    using Sentence-BERT from the sentence-transformers library.\n",
    "    \"\"\"\n",
    "    enc_reviews = [None] * len(reviews)\n",
    "    cum_sum_sentences = [0]\n",
    "    sent_count = 0\n",
    "    \n",
    "    # Flatten reviews into a list of sentences\n",
    "    for review in reviews:\n",
    "        sent_count += len(review)\n",
    "        cum_sum_sentences.append(sent_count)\n",
    "\n",
    "    all_sentences = [sent for review in reviews for sent in review]\n",
    "    print('Loading pre-trained Sentence-BERT model...')\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "    # Group back encoded sentences by reviews\n",
    "    for i in range(len(reviews)):\n",
    "        begin = cum_sum_sentences[i]\n",
    "        end = cum_sum_sentences[i+1]\n",
    "        enc_reviews[i] = enc_sentences[begin:end]\n",
    "        \n",
    "    return enc_reviews\n",
    "        \n",
    "    \n",
    "def summarize(reviews):\n",
    "    \"\"\"\n",
    "    Performs summarization of book reviews.\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    summary = [None] * n_reviews\n",
    "    print('Preprocessing...')\n",
    "    preprocess(reviews)\n",
    "    \n",
    "    print('Splitting into sentences...')\n",
    "    split_sentences(reviews)\n",
    "    \n",
    "    print('Starting to encode...')\n",
    "    enc_reviews = encode_sentences(reviews)\n",
    "    print('Encoding Finished')\n",
    "    \n",
    "    for i in range(n_reviews):\n",
    "        enc_review = enc_reviews[i]\n",
    "        n_clusters = int(np.ceil(len(enc_review) ** 0.5))  # Number of clusters\n",
    "        \n",
    "        # Perform KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "        kmeans = kmeans.fit(enc_review)\n",
    "        \n",
    "        avg = []\n",
    "        closest = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_review)\n",
    "        \n",
    "        # Ordering sentences by clusters\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        summary[i] = ' '.join([reviews[i][closest[idx]] for idx in ordering])\n",
    "    \n",
    "    print('Clustering Finished')\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab984d2a-8178-4f5a-92da-f0a4b3fc6a0a",
   "metadata": {},
   "source": [
    "#### What is happening here?\n",
    "Encoding:\n",
    "- Sentence-BERT: The function uses a pre-trained Sentence-BERT model ('all-MiniLM-L6-v2') to convert each sentence into a vector embedding. - This embedding is a numerical representation of the sentence that captures its semantic meaning.\n",
    "- It first flattens all the sentences from the reviews into a single list and then encodes them.\n",
    "- After encoding, it restructures the embeddings back into their original review groups.\n",
    "\n",
    "Clustering:\n",
    "- For each review, the number of clusters is determined using the square root of the number of sentences (rounded up).\n",
    "- KMeans clustering is performed on the sentence embeddings to group similar sentences.\n",
    "- For each cluster, **the sentences closest to the cluster center (based on the distance between sentence embeddings) are selected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a344d-e052-411e-b037-9b719a5b1e68",
   "metadata": {},
   "source": [
    "### Applying to a single book's review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2739a959-7643-41b1-9e23-2e21c952772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_subset = reviews[reviews['Book Title'] == 'Ways of Seeing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c0363ed2-5433-4a0a-9d14-59d2ee3f7dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 116\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec944afd15e6430b940f7e69b95be4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Finished\n"
     ]
    }
   ],
   "source": [
    "way_of_seeing_reviews = reviews_subset['Review Text'].tolist()\n",
    "summaries = summarize(way_of_seeing_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c19353f4-f1bd-424e-9468-9391ce4ad9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In general, the lesson of this book is that all art is bad for you, except the pieces that the authors of this book like. I am not the audience for this book, mainly because I've already read and more or less digested the handful of essays and ideas on which it is based. This is true. They don't discuss the 20th century at all (I know they know that twentieth century art exists; perhaps, as good Benjaminian Marxists, they don't like abstraction or difficulty). Holbein's 'Ambassadors' is read as an example of this; the incredible distorted skull in the painting is the exception which proves the rule of oil paintings rather than, you know, showing that oil paintings can be self-critical, as are most good artworks of any kind. Harmful because those who accept it will say silly things, and because those who read it and reject it out of hand (due to the rhetoric, bad arguments, or conceptual confusion) won't be challenged to, you know, care about other people.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3928fac9-10b9-46fc-bc7b-98d18b378739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not the audience for this book, mainly because I\\'ve already read and more or less digested the handful of essays and ideas on which it is based. The seven chapters break down fairly simply. 1: Benjamin\\'s \\'Work of Art\\'--the ability to reproduce images alters the way we encounter works of art. This seems reasonable. Nobody gets to see a Giotto without having seen a reproduction first, except someone who has no interest in the Giotto in the first place. But Berger et al* go a step further: we need to use the fact that we encounter works of art differently to undermine the ruling class\\'s privilege and the \"specialized experts who are the clerks of the nostalgia of a ruling class in decline.\" That\\'s on page 32. Part of me, a large part, laments the fact that you\\'d never get that published today, not even on a website. Another part of me laments the stupidity of intellectuals who put their faith in the inherent goodness of The People. The People does not have a good track record when it comes to art appreciation. That\\'s not to say that people can\\'t learn to appreciate art, only that We are no better and no worse than the ruling class was. We need to learn, we need to be taught, you can\\'t do that if you assume that We are inherently able to do the right thing. 2 & 3: Women are depicted differently from men, and, frankly, not in ways that are healthy for anyone, but particularly not for women. I agree. Which makes it breathtaking to see the authors get so many things wrong, either intentionally (cutting short the bible verse in which God punishes Eve *and Adam*); stupidly (non-Western art forms show women as active participants in sex, so that are isn\\'t morally dubious); or in ways that are, ahem, temporally bound (\"Hair is associated with sexual power, with passion.\" Seventies!). 5: Oil paintings are bourgeois and generally not morally okay. Holbein\\'s \\'Ambassadors\\' is read as an example of this; the incredible distorted skull in the painting is the exception which proves the rule of oil paintings rather than, you know, showing that oil paintings can be self-critical, as are most good artworks of any kind. In general, the lesson of this book is that all art is bad for you, except the pieces that the authors of this book like. They like pieces by artists who can plausibly be turned into radicals, because only radicals can be interesting (Franz Hals; William Blake). They don\\'t discuss the 20th century at all (I know they know that twentieth century art exists; perhaps, as good Benjaminian Marxists, they don\\'t like abstraction or difficulty). They\\'re also very uncomfortable with religious art, and want to group, e.g., Ambrosius Benson\\'s Mary Magdalene with the absurd and/or pornographic Magdalene of later times, rather than admitting the rather obvious differences (Benson\\'s is rich, but not, how can I put this... naked and disheveled.) Since the authors have a hard time saying what they actually like (vs. what they suspect is oppressive), you get idiocies like this: Rembrandt\\'s famous late portrait shows a man for whom \"all has gone except a sense of the question of existence, of existence as a question.\" A little thought would show that this is the sort of conservative pablum Great Artists have been serving up for generations. 6 & 7: Advertizing uses art to make you think you want things you don\\'t want and that you can get them, so you don\\'t need to think about what you really want, e.g., more time away from the office. This is true. In sum: I was sucked in by the idea that this was a book about understanding art. It is not. It is critical theory for high-school readers. Good for what it is, but extremely narrow in scope, and quite harmful for anyone who swallows it whole rather than taking a few minutes to worry away at its assumptions. Harmful because those who accept it will say silly things, and because those who read it and reject it out of hand (due to the rhetoric, bad arguments, or conceptual confusion) won\\'t be challenged to, you know, care about other people. * Humorous aspect of this book: it makes a big deal about how it was written by a group of people, because, you know, individuals are bad, and groups are good. You\\'ll note that the book is sold as a book by John Berger. You can draw the conclusion.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Review Text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9120079-d956-4cbd-9952-acb1f8a738c0",
   "metadata": {},
   "source": [
    "## Full Book Review Summary Generation\n",
    "Now with the ability to create summaries for each review individually, we can generate one last summary - one that describes how most readers perceive the book (themes, flaws, strengths, etc). First, let's generate another set of summarized reviews.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b778a-c6b7-4cbc-b5d6-a42cd1a8d6c0",
   "metadata": {},
   "source": [
    "#### Applying Review Summaries to Born A Crime by Trevor Noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8c2c6f4-62c7-49e0-a007-ded060e261a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5775d1c968b44a53ae9eb2ae48fa52f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    }
   ],
   "source": [
    "born_a_crime_subset = reviews[reviews['Book Title'] == 'Born a Crime: Stories From a South African Childhood']\n",
    "born_a_crime_reviews = born_a_crime_subset['Review Text'].tolist()\n",
    "born_a_crime_summaries = summarize(born_a_crime_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be1f3afc-c7af-4773-b48b-d4d3b63401d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noah is very wise to be so young, one of my favorite quotes from the book is \"The hood was strangely comforting, but comfort can be dangerous. There were different rules for whites, colored (people descended from the first white settlers and the natives), blacks, Indians and Asians. One of the many things I didn\\'t know about South Africa is that as most countries were trying to fix racist policies after the World War 2 holocaust had shown us where discrimination could lead, the South Africans or Afrikaners (as the Dutch colonists called themselves) were running towards institutionalized racism. Comfort provides a floor but also a ceiling.\"'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_summaries[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b960995d-e9d6-4593-b3e4-f50d9997c731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the best memoirs I've ever read, Born a Crime is so funny and wise and sad and reveals much about the writer.\",\n",
       " \"Trevor Noah is an exceptional young man raised by a strong, formidable mother who never gave up on him and loved him with a fierce pride.He never shrinks from complete honesty in the telling, even in those areas that don't reflect well on his teenaged and early adult self.\",\n",
       " 'One very amusing incident tells of how as a kid his rejection of going to the outside toilet in the rain led to a most unfortunate incident with his blind grandma.',\n",
       " \"Like in the best books, you learn some important history in the telling of his young life's story.\",\n",
       " 'Apartheid was made officially part of South African government in 1948; whereas, before you had casual, implied racism, now it was a system of specific laws that rated you as a person.',\n",
       " 'There were different rules for whites, colored (people descended from the first white settlers and the natives), blacks, Indians and Asians.',\n",
       " 'Blacks were at the bottom with prison time for those who produced children from a black and white union.',\n",
       " 'Hence the title, Born a Crime, is stating a fact of life in the nation of South Africa at that time.',\n",
       " \"One of the many things I didn't know about South Africa is that as most countries were trying to fix racist policies after the World War 2 holocaust had shown us where discrimination could lead, the South Africans or Afrikaners (as the Dutch colonists called themselves) were running towards institutionalized racism.\",\n",
       " 'The Afrikaners were fans of Hitler and actually formed a committee to study racism around the world and pick out the\"best\" bits for themselves.',\n",
       " 'They studied Australia, the Netherlands and the US.',\n",
       " 'Noah sums it up very well: in America we had 1) forced removal of the native population to reservations, 2) slavery, followed by 3) segregation.',\n",
       " 'In South Africa all 3 were done at the same time to the same people.',\n",
       " 'Noah is very wise to be so young, one of my favorite quotes from the book is \"The hood was strangely comforting, but comfort can be dangerous.',\n",
       " 'Comfort provides a floor but also a ceiling.\"',\n",
       " \"This man's talent knows no ceiling.\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_reviews[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a6b93-2b67-4841-a290-fca9cece6188",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So far so good - the summaries skip over some information here and there, but at a glance, they look quite accurate. Now let's take it a step further and generate a final summary that should encapsulate the core themes and ideas mentioned in the reviews.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef5ac854-854a-4031-a317-a3bd7144e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_final(all_summaries):\n",
    "    \"\"\"\n",
    "    Performs summarization of the combined summaries.\n",
    "    \"\"\"\n",
    "    all_summaries = [summary.replace('\\n', ' ').strip() for summary in all_summaries]\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    all_sentences = []\n",
    "    for summary in all_summaries:\n",
    "        sentences = sent_tokenize(summary)\n",
    "        sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        all_sentences.extend(sentences)\n",
    "    \n",
    "    print('Loading pre-trained Sentence-BERT model...')\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "    n_clusters = int(np.ceil(len(enc_sentences) ** 0.5))\n",
    "    print('Clustering sentences...')\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(enc_sentences)\n",
    "    \n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_sentences)\n",
    "    \n",
    "    ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "    final_summary = ' '.join([all_sentences[closest[idx]] for idx in ordering])\n",
    "    \n",
    "    print('Final summary generated')\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2873e671-cec5-4b06-80f0-48c2867e0ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a73be608224234be7d499f7ec75c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    }
   ],
   "source": [
    "final_born_a_crime_summary = summarize_final(born_a_crime_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdb17962-acfb-4ddd-8e8d-17d075a05ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While Noah may just be a generally bubbly and optimistic person - which I respect a lot, considering the hardships he's overcome - there were chapters in which I wanted more introspection, more nuance, and more emotional vulnerability. Eventually things turned bad when he became involved in petty crimes and got in trouble with the law. Many reviews have already been written for this book but I just wanted to add my feelings about the audiobook. Trevor Noah has a very interesting life story to tell, and along the way many readers such as myself learn a great deal about South Africa, Noah's place of birth. I NEEDED to know what happened. Trevor Noah is a remarkable person. So many of Trevor's stories revolved around his mother, and with good reason - she raised him alone (mostly), and so she was a huge influence on and guiding force of his life. 4.5 stars “Language, even more than color, defines who you are to people.” Born a Crime is a fascinating insight into the racial tensions still very much alive in South Africa and the struggles and achievements of living in impoverished areas just outside Johannesburg. reading this made me realize that i am grievously uneducated on apartheid. I recommend this book to absolutely everyone, I got sooo much more than I expected from it. He doesn't glorify himself and doesn't shy from describing certain flaws in himself, his family and the world he grew up in, which makes it seem very real and believable, and makes me wish I knew even more. ! is filled with heart, humor, trauma, and tears. Noah knew no other way of life & sees racism in all its many facets as a routine way of life. There is so much that we can learn from the stories that he shares. His mother was a strong, independent woman in a place that didn’t appreciate or encourage that. Trevor Noah soon became one of my favourite comedians; I watched his 2 Netflix specials and also followed The Daily Show on a regular basis. As well, the book tells us of the role his remarkable mother played in contributing to his success. But is about real things: racism, religion, poverty, domestic violence, and so much more. I LOVED this book! So when I read this book, there are some jokes that I have heard before in his stand-up, but never mind; it is still funny as hell to hear it twice, thrice, or even more. Outstanding, brilliant, hilarious, profound memoir of his childhood in South Africa.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_born_a_crime_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9e95f-8275-4ed7-85da-2135d4a7f06e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This final generated summary mentions essentially every aspect that you can find in most reviews (audiobook being better, Noah's relationship with his mother, apartheid and the disconnect that some readers have with it, etc). \n",
    "\n",
    "---\n",
    "\n",
    "For the sake of coherency, we can now parse this final summary to `facebook/bart-large-cnn` with a prompt to reframe the summary as a book description.\n",
    "\n",
    "`facebook/bart-large-cnn` is a fine-tuned version of BART (Bidirectional and Auto-Regressive Transformer), a transformer-based sequence-to-sequence model introduced by Facebook AI, combining the advantages of bidirectional and auto-regressive models. The model uses an encoder-decoder architecture: the encoder processes the entire input (like BERT), and the decoder generates output token-by-token (like GPT).\n",
    "\n",
    "Pretrained on various denoising tasks to learn language patterns, BART was fine-tuned specifically on the CNN/DailyMail dataset to improve its summarization capabilities. During fine-tuning, the decoder attends to the encoder's output to generate contextually appropriate summaries (Attention Mechanisms). Beam search and length penalty were also used to avoid overly short summaries.\n",
    "\n",
    "The weights were initialized from the original BART model that was pretrained on a large corpus of text using the denoising autoencoding tasks mentioned earlier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d617e5b9-d7bd-45da-9573-655ac5865541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_book_description(summary):\n",
    "    \"\"\"\n",
    "    Takes the final summary of reviews and generates a book description.\n",
    "    \"\"\"\n",
    "    description_pipeline = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "    prompt = (\n",
    "        \"Convert the following summary into a structured book description that describes the book's content, \"\n",
    "        \"themes, and significance: \" + summary\n",
    "    )\n",
    "\n",
    "    result = description_pipeline(prompt, max_length=250, min_length=100, do_sample=False)\n",
    "\n",
    "    book_description = result[0]['generated_text']\n",
    "    #print(book_description)\n",
    "    return book_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "42560d2c-6553-4d1b-a2a3-c32329e09bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_description = generate_book_description(final_born_a_crime_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5f30216-7aeb-4e4d-a1b6-ad65ea2d5af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trevor Noah's love and respect for his mother & the way she raised him shines through on nearly every page. Eye-opening and perspective changing in a way that's funny and deeply vulnerable, you'll feel educated and entertained at the same time. For an enhanced experience, I highly recommend the audiobook version. Moved out of the house at the age of 17 because of his step-dad and was even jailed for using a fake license plate. Imagine being born from a black mother and a white father in a country where interracial relationships were against the law.\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718baf32-c6d4-4668-9da0-31ccf9114396",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can now apply this to any book we like to obtain an overview/description of the contents and themes soley based off of the reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26b08b-e114-4ee6-b6a5-510971b601e1",
   "metadata": {},
   "source": [
    "## Generating Final Summaries For 10 Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14c25d3b-5449-4a00-90ea-93db13b08ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wind in the Willows\n",
      "The Voyage of the Dawn Treader\n",
      "Drive: The Surprising Truth About What Motivates Us\n",
      "Team of Rivals: The Political Genius of Abraham Lincoln\n",
      "The Art Forger\n",
      "Winnie-the-Pooh\n",
      "The Lean Startup (Hardcover)\n",
      "Harry Potter and the Prisoner of Azkaban\n",
      "Goodnight Moon\n",
      "Vincent Van Gogh: The Complete Paintings\n",
      "Charlie and the Chocolate Factory\n",
      "Anne of Green Gables\n",
      "Alexander Hamilton\n",
      "Wild: From Lost to Found on the Pacific Crest Trail\n",
      "Purple Cow: Transform Your Business by Being Remarkable\n",
      "Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers\n",
      "On Photography\n",
      "The Cost of Discipleship\n",
      "The Work of Art in the Age of Its Technological Reproducibility, and Other Writings on Media\n",
      "Concerning the Spiritual in Art\n",
      "\n",
      "\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e57384a07749769eebfdbf92893a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271289645aee409bbc7e13666c0f4eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 240\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5828bc90de86462e900158bf8dce357d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a5df21b4b9479e838cef5470da07e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 119\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3c8e3030c74d87b97aa4818035d30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e7da7546b146b890f19ea98802cf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a7fc4913cc4110877595d57ec7cb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94ddf9425b44701967e3210dda8b7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e819b2425ab44ef9b6f3a707cd319b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1741b7eb684993a7a753c7c4c2ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce42318bf45b45739cbcb55649aa93fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a8017ea0b645398525b76d7f4e4f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e5822f19647d499bb801cb18b759f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db54b974edcf42cbbc7d679ed6e80479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ba5179d58f46ecb4c6948e0a48f9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e44ca38eb2440e8789690fd05a3cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4eeca12dea34eab8d1cb09ae8605a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbd95121b00421a9040daba75418472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed53af9392d464aa389a1aa3b6da035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13ee32f08e4e99a7961ddb5e482dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b053155011cb485fab0a048d355dae2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b36280dc8f24b149ad5e7016d30754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 119\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d6d8e76a5045ad8f14006486fa7413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc476ee4d16d4d2abc021bc5e03fd71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134cf3e84b764b45a3120cf7fe6301d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741fdca5a8f54f62ad3c937a664d29d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 119\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc06ac87113245309da24912ce500764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e53093e4144e3c93c513ad21b249da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fbd40a677d408397b8aaadcaf32525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f880bf1a61ed460ead264b1dde774c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8650e2c75a7e4f1caca8bc7fff83e3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0156497092d245ac9ec60cb5f3d5da55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 119\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cfd3efd8974dd79f5c5deb007ff6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eef5b9ebc5c4e458f3935e10be8c9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 120\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a49b682a764b6fb3f2d333270fdee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cecb4e317647fab2894104c77e1635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85be0833bf2646e4a135428a248487d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede892083774875986615d18d45208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "Preprocessing...\n",
      "Number of reviews: 119\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a5716426364497be4ae7691cec1bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1dddab2e1c4148be5c1e86d20d1aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n",
      "\n",
      "Total processing time: 1374.19 seconds\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Wind in the Willows\n",
      "Generated Description:\n",
      "This is, then, a cautionary tale, a warning to the propertied classes to take up, if necessary, arms against the lower classes and to stop living lives of decadent indolence. A meditative outstanding forest joyride which professes loyalty and value of friendship, ramifications of greed and robbery, misjudgments and reverence, conceit and vanity, and above all - animal etiquettes. The protagonists, our incorrigible and exasperating toad, the loyal and responsible friends, the water rat and ever-gadding mole, and finally our revered badger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Voyage of the Dawn Treader\n",
      "Generated Description:\n",
      "My favorite in the series so far! I love the whole Narnia series, but this was my favorite. I had such a literary crush on Caspian, and I'd got a lot fonder of Edmund, too. The story is fun for children of all ages if you don't mind the overtness of the Christian allegory in this one. And of course there's the part about Eustace and the dragon, which involves transparently Christian imagery but it works better in this novel than in some of the others.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Drive: The Surprising Truth About What Motivates Us\n",
      "Generated Description:\n",
      "Drive takes a deeper look at businesses and owners that that use \"motivation 3.0\" to appeal towards employees real drives. It offers ideas on how people can inject real motivation in all aspects of their lives: work, child-rearing, personal life, and more. The book is full of practical and interesting examples that not only makes the book fun to read but helps the author lay out his thoughts and vision on motivation. I imagine this is a great book to confuse those with a lot of management theory behind them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Team of Rivals: The Political Genius of Abraham Lincoln\n",
      "Generated Description:\n",
      "\"Team of Rivals\" is both an entertaining story and a valuable source of insight into Lincoln - and not to be missed. Doris Kearns Goodwin spends equal amounts of time on Seward, Chase, Stanton et al in order to give the reader a great feel for just how amazing Lincoln was. I dreaded reaching the last pages of this book--there was only one way for it to end. This book is an example of why I like reading history. If you are a fan of Lincoln, this book is a must.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Art Forger\n",
      "Generated Description:\n",
      "The main character was just boring, and I never understood where she was coming from and what motivated her. The most vivid character is the Isabella Stewart Gardner museum, and only because I have visited it several times and love it. If you like art, you’ll love this book. Overall, the book was enjoyable. Such a shame for the art world. Could give to this book a higher rating but the style of writing is really appalling. I'm firmly in the \"liked it but didn't love it\" category.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Winnie-the-Pooh\n",
      "Generated Description:\n",
      "A really wonderful collection of short tales that introduces the friends of Christopher Robin. My favorite part is Chapter Three \"IN WHICH Pooh and Piglet Go Hunting and Nearly Catch a Woozle.\" Simply delightful! There is so much commentary on human nature, and I can see myself in all of the characters in some way. Read aloud to Oscar, who giggled at all the right moments (and sometimes at the wrong ones, but he’s only 5 months old). Like, A.A Milne is not allowed to make me feel these feels in the form of a children's story book! Yes, yes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Lean Startup (Hardcover)\n",
      "Generated Description:\n",
      "Zero to One by Peter Thiel asserts this position. It got a bit repetitive for me, but the crux of the book is very profound. Unfortunately, we are delivering faster and faster things that our customers often do not need. Measuring (validate hypotheses) as we build is not easy, as it can bring much waste when startups get trapped in the perfectionism paradigm. Loved this book since it contained a lot of highly informative and applicable concepts that can be adopted in a startup in order to maximize the odds of success.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Harry Potter and the Prisoner of Azkaban\n",
      "Generated Description:\n",
      "This is the book when we start to see so many pieces that Rowling has laid out in the first two novels fall into place. Trust me, this book is the last book that will leave you feel whole after you finish it. Overall, just so great. The movie is wonderful, but so many things are different in the book. I find the whole Marauder's story kind of tragic - with Lupin & Sirius losing their best friend (and, due to Peter's betrayal, 13 years worth of time with Harry)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Goodnight Moon\n",
      "Generated Description:\n",
      "A young anthropomorphic bunny is in bed saying to everything she can see around her: There are three books in the series, all by the same author and illustrator. Ugh, this book is horribly illustrated, garishly colored and not very creative. I would give this read one star but Neil said that due to the physical laws of the universe I can neither give nor receive stars Goodnight moon. Loved it! If you are interested, here is an excerpt from it: P.S. The perfect book for bedtime for littles.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Vincent Van Gogh: The Complete Paintings\n",
      "Generated Description:\n",
      "This book is amazing: it collects all his surviving paintings, in overall good quality reproductions. I sympathize with the fact that he had a mental illness and used his art to express himself. The book has wonderfully deep detail on Van Gogh and the individual works he did. The text is okay, general opinions on his artistic style along with some quotations from his letters and facts about life. I found it fascinating to see the different stages in his life and to how many different art styles he committed before finding his own. Simply beautiful.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Charlie and the Chocolate Factory\n",
      "Generated Description:\n",
      "Charlie is captivated by his grandfather's stories about Willy Wonka’s mysterious chocolate factory, so he is excited when he wins one of the tickets to tour it. What I love about this book are the lessons learned from the story of each of the four children who were \"eliminated\" because of their behaviour during their stay at the factory. Seems like this crazy genius couldn't care less about the implications of insulting children - or adults, for that matters - for being fat, ugly or spoiled; or for matching any of his pet peeves (chewing a gum, watching the telly, owning toy guns)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Anne of Green Gables\n",
      "Generated Description:\n",
      "As an adult, I still wholeheartedly adore this book! I love Anne so much. Her heart’s in the right place and she cares for every single person she holds dearly with such deep devotion it nearly rips her apart at times (which I strongly relate to). It nearly crushed me when Anne fell in love with Green Gables only to find that she must surely be turned away and carted straight back to the orphanage. I wish I'd read it sooner, though I'm glad to have finally read it. But never forgetting the days of blissful youth, lingering still as she matures.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Alexander Hamilton\n",
      "Generated Description:\n",
      "Ron Chernow's biography paints the details of Hamilton's life with a vision of just how incredible a figure Hamilton was, and how his talents often unsettled those around him. The author sometimes went so far in depth in creating supporting characters histories that I felt, at times, that I was reading someone else's biography. Chernow relates that \"Only once did Burr betray any misgivings about killing Hamilton. He really made the Founding Fathers seem as they are alive. He also foreshadowed to the duel WAY too much\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Wild: From Lost to Found on the Pacific Crest Trail\n",
      "Generated Description:\n",
      "Cheryl lost her mother when she was in her early twenties. She decided to do what anyone would do, she decided to hike the Pacific Crest Trail. The writing is GRAND, and never have I wanted to go cross-country hiking so fervently as I do now (though I probably never would). So far, a great read. I felt like I was with her every step of the way as she recounted her trip in this novel. I never felt any connection to Strayed and her struggles because the writing is merely adequate.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Purple Cow: Transform Your Business by Being Remarkable\n",
      "Generated Description:\n",
      "Seth Godin's book is a light read and factoring that it was published quite a few decades ago, some of the ideas will seem outdated. Even if it's obvious today, these ideas are written in the beginning of 2000s which makes it remarkable and puts the author ahead of its time to follow closely. It's a 3-hour audiobook, and even at 2x speed, it would have been better suited to a TED Talk. Find a way to make your product/business remarkable. Easy and quick read.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers\n",
      "Generated Description:\n",
      "The author takes a deep dive into the frequently overlooked mechanics of what gets a successful product from inception to sustained success at each segment of the adoption curve. The insights provided are not just about selling but about making a difference in the lives of customers by addressing their real needs and desires. An interesting read for anyone who is in marketing. Highly recommended for any executive of a startup company looking for guidance in the face of so many questions that arise from an early market success. This book was included in my book: The 100 Best Business Books of All Time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "On Photography\n",
      "Generated Description:\n",
      "Sontag explores the role of photography in the realm of the domestic American family. Continues to be relevant today, perhaps more than ever with the digital photography revolution well underway. \"Photography does not simply reproduce the real, it recycles it.\" Every paragraph sounds like the end of a chapter, a summation filled with nuance and aphorism, but it's too much. I gave it 4 stars, it's an immersive read. They age, plagued by the usual ills of paper objects; they disappear; they become valuable, and get bought and sold.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Cost of Discipleship\n",
      "Generated Description:\n",
      "Bonhoeffer is an amazing writer and theologian. This book was deeply challenging to me personally, and I'm thankful that I had the chance to be exposed to it. There were several chunks I would argue against his interpretation, namely his beliefs about the extent of baptism. This probably isn't the best book for a non-Christian or someone who has been away from church a while. If we lose sight of that and exchange it with a focus on external obedience to a law we can't obey we will be forever chasing a salvation that's already been offered.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "The Work of Art in the Age of Its Technological Reproducibility, and Other Writings on Media\n",
      "Generated Description:\n",
      "Benjamin's essay was written in 1939, before the rise of television and the internet. The work of art becomes merely another commodity, eyed by the masses, and ceasing to have a physical history. This is the situation of politics which Fascism is rendering aesthetic. The very term “aura” implies that its referent is something that lies not within the artwork, but around it. The spectator’s process of association in view of these images is indeed interrupted by their constant, sudden change.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book Title:\n",
      "Concerning the Spiritual in Art\n",
      "Generated Description:\n",
      "Fascinating perspectives. It's full of a unique kind of wisdom, and although Kandinsky's contemporary Paul Klee is arguably the more influential of the two in terms of art history, this book is a gem. It was a fast read and interesting so it was worth my time to read this one. A very insightful approach on art. Well the good news is I purchase the house last month cause TROVIAN TEAM eventually deleted the car loan and raised my score to 810 in less than 7days including the negative items on my report was cleared.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start timer for the whole process\n",
    "total_start_time = time.time()\n",
    "\n",
    "unique_books = reviews['Book Title'].unique()\n",
    "random_books = random.sample(list(unique_books), 20)\n",
    "book_descriptions_dict = {}\n",
    "\n",
    "for book in random_books:\n",
    "    print(book)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for book in random_books:\n",
    "\n",
    "    book_subset = reviews[reviews['Book Title'] == book]\n",
    "\n",
    "    book_reviews = book_subset['Review Text'].dropna().tolist()\n",
    "\n",
    "    if not book_reviews:\n",
    "        continue\n",
    "        \n",
    "    book_summaries = summarize(book_reviews)\n",
    "    final_book_summary = summarize_final(book_summaries)\n",
    "\n",
    "    book_description = generate_book_description(final_book_summary)\n",
    "\n",
    "    book_descriptions_dict[book] = book_description\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"\\nTotal processing time: {total_duration:.2f} seconds\")\n",
    "\n",
    "for book, description in book_descriptions_dict.items():\n",
    "    print(\"\\n\")\n",
    "    print(f\"Book Title:\\n{book}\")\n",
    "    print(f\"Generated Description:\\n{description}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002899f3-9362-4d37-974e-ca577a06d2e5",
   "metadata": {},
   "source": [
    "---\n",
    "This process (obviously) took quite a while. To remedy this, we can make use of cloud computing in the form of Google Colab. Using their T4 GPU, it only took 2 minutes (almost 10x faster) to generate the following for another 20 random books:\n",
    "\n",
    "Total processing time: **141.88 seconds**\n",
    "\n",
    "**Book Title:**<br>\n",
    "Zero to One: Notes on Startups, or How to Build the Future<br>\n",
    "**Generated Description:**<br>\n",
    "Peter Thiel is a massive contrarian (and I'm against extreme contrarianism and extreme conformity), but his ideas are grounded and logical. He claims, for example, that nothing but information technology and communications technology have evolved much since the 70s. Instead of trying to find a niche in a monopolistic market, startups are supposed to find entirely different ideas. He also provides a number of other areas of thought for the entrepreneur when starting and running a business. I wouldn't generalize Thiel's wisdom to fields outside of startups (just like the case with Paul Graham) -- indeed he made some claims that were not well thought out.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Autobiography of Malcolm X<br>\n",
    "**Generated Description:**<br>\n",
    "Malcolm X's life was a struggle. May Allah forgive all his shortcomings and allow us to spread Islam with even a morsel or his enthusiasm, determination and commitment. It is a long and powerful story that reveals another facet of the fight for racial justice and equality. My copy of this book is already in tatters for the work I have done on it but I will buy a new one, frame it perhaps in a glass ceiling, and look at it day and night just to remind myself what a dedicated man can become but most importantly what he can overcome.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Bonhoeffer: Pastor, Martyr, Prophet, Spy<br>\n",
    "**Generated Description:**<br>\n",
    "The author Eric Metaxas must have spent years researching for this book. Bonhoeffer is an example of a man who loved the Word of God, the Church, and the person of Christ. His ideas/ theology. Was 3-stars, now 2-Stars Comment 20 September, 2023: I first read this book sometime in 2010-2011 - maybe prior to joining Goodreads, but certainly before I commenced writing any reviews. The book is so well-written and compelling. A fascinating life.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Surprised by Joy: The Shape of My Early Life<br>\n",
    "**Generated Description:**<br>\n",
    "\"I was equally angry with Him for creating a world.\" A young man who wishes to remain a sound Atheist cannot be too careful of his reading. A fairly dull middle, sandwiched between an interesting start and an interesting end. The book deepened my understanding of and appreciation for Lewis as a person, scholar, and author (not to mention adding new layers and depth of his other books). Reading how his life effected what he thought and wrote about is truly interesting. But I really loved this book.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Agony and the Ecstasy<br>\n",
    "**Generated Description:**<br>\n",
    "Author Irving Stone is a fantastic writer. I found myself reading slower and slower towards the end, because I did not want to finish reading the book! I felt the author devoted enough time to each event in Michelangelo's life to give it meaning and purpose, but was sure to move on when it was time. By the end of the book, you will feel that you know Michelangelo: family issues, rivalries, the popes, his friends, and his delight and obsession for his art. It also gives a excellent account of the the history and life in Italy in the 16th century.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Color and Light: A Guide for the Realist Painter (Volume 2)<br>\n",
    "**Generated Description:**<br>\n",
    "James Gurney's book is a clear and concise collection of information about the science of light color as they relate to observational (and imagined!) The lessons are observations of light and colors from James Gurny's years of painting experience. Convert the following summary into a structured book description that describes the book's content, themes, and significance: I learned more about color theory reading this book than I did studying illustration for two years at Pratt Institute. I will definitely be revisiting this book often!\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Art Thief: A True Story of Love, Crime, and a Dangerous Obsession<br>\n",
    "**Generated Description:**<br>\n",
    "This true story of a one of a kind criminal reads like fiction and I thoroughly enjoyed it. Police estimates he's stolen between $1 to 2 billion worth of art. He steals because he loves art. Obsession, madness, narcissism. He lived in an attic in his mother’s house with Anne-Catherine for most of his life where he accumulated his stolen art like a pack-rat. I could not stop listening to this audiobook! Nonfiction account of a prolific art heist.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Shoe Dog: A Memoir by the Creator of Nike<br>\n",
    "**Generated Description:**<br>\n",
    "The book is unexpectedly enjoyable to read - well written and intriguing - and is not your standard entrepreneurship story. The story of Phil Knight is everything books like \"Art of the Deal\" are not: the humble story of a dream, framed more as an accidental life journey than the story of rags to riches. He came off too proud, full of elitism and arrogance, he’s mysoginistic and extremely privileged. HOWEVER this book was pretty good and inspirational - message to work hard and believe in yourself.\n",
    "\n",
    "**Book Title:**<br>\n",
    "How to Win Friends & Influence People<br>\n",
    "**Generated Description:**<br>\n",
    "D.C. by Carnegie makes following his principles sound too easy. I think I expected a lot of dated advice since this was published so long ago, but I found myself enjoying this and also understanding why it remains so popular after so many decades. Some other things give me this weird feeling of 'fake superficiality' and it decribes a world where people can't disagree or be frank with eachother. But that's really just me trying to find something positive (using the \"principles\") in a book that I am still trying to UNlearn.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Anne of Green Gables<br>\n",
    "**Generated Description:**<br>\n",
    "This book made me cry. My daughter loved this book over the summer, one of the classics I’ve never read and it’s now firmly on my To Read list. I loved the writing so much and was surprised how lovely it is. Marilla and Rachel were great as well and I feel like Marilla had the most satisfying character development. This will be my comfort read, I'm obsessed and can't wait to watch the show Also Gilbert has my whole heart, so do all the other characters omg\n",
    "\n",
    "**Book Title:**\n",
    "The Practice of the Presence of God<br>\n",
    "**Generated Description:**<br>\n",
    "The life of Brother Lawrence is testimony to his writings; his single-minded concern for God, far from leading him away from love of people, brought him closer to them. The book is less practical than I was expecting, in terms of specific tips for spiritual disciplines. There were also times when his theology seemed a bit heavy on the idea that God sends all inflictions and suffering and sickness to purge/cure our soul. “We ought not to be weary of doing little things for the love of God, who regards not the greatness of the work, but with which it is performed”\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Horse and His Boy<br>\n",
    "**Generated Description:**<br>\n",
    "This is by far the best Narnia book. As a lover of redemption arcs, I find this story very satisfying. I love the relationship between Shasta and the talking horse, Bree. So many good scenes with Aslan. Several of his moments moved me almost to tears. The prince agrees, because he's so certain his evil plan will work. And yes, I love it. But a boy in battle is a danger only to his own side. I should have read this book sooner >.<\n",
    "\n",
    "**Book Title:**<br>\n",
    "Alexander Hamilton<br>\n",
    "**Generated Description:**<br>\n",
    "At over 800 pages, it’s a hefty read, and there were times when I felt bogged down by the sheer volume of detail. The author sometimes went so far in depth in creating supporting characters histories that I felt, at times, that I was reading someone else's biography. It's dense and has some passages that are somewhat dull, and even the succinct writing style of Chernow didn't ease it much. I would only recommend this to history buffs and someone researching Hamilton's life. Though it took 36 hours, it engaged my interest throughout.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Dreams from My Father: A Story of Race and Inheritance<br>\n",
    "**Generated Description:**<br>\n",
    "The book traces Obama's quest for self and purports that as a mixed race person with his family ties split between two continents and two cultures, he is confused and torn. I both understood and was puzzled by some of his feelings of loathing and anger towards himself and US society. I personally could connect with various aspects of his struggle and the larger struggle of the black community. The Kenya story is beautiful, but becomes a telling by his grandmother for 40 to 50 pages. I’m giving it 4 stars only because I felt as though the middle portion of the book dragged on a bit.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Color: A Natural History of the Palette<br>\n",
    "**Generated Description:**<br>\n",
    "Victoria Finlay is a very interesting person. She is lucky, too late for me to ever see them now, the extent of her journeys in the book is remarkable. I was expecting to read more of a history book, but it turned out to be a travelogue/memoir. The author is definitely part of the story, but for me, it lent a human aspect and interest that may otherwise have been lacking. Although a fan of micro-histories could certainly enjoy this book, they shouldn’t go in expecting a Simon Winchester style non fiction book.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Art Book<br>\n",
    "**Generated Description:**<br>\n",
    "This isn't an in-depth exploration. The pictures are of a very good quality and the book itself is a monster. Lots of diversity in terms of styles of art shown (not so much diversity in the artists) This is a wonderful book for art fans, as well as those who are art curious. Not for the academics, but that's part of the appeal. I used to flip through a copy of this and the companion Photography book at Superstore while my Mom shopped for groceries.\n",
    "\n",
    "**Book Title:**<br>\n",
    "The Intelligent Investor<br>\n",
    "**Generated Description:**<br>\n",
    "Graham is known for inspiring Warren Buffett, and many other major investors. He goes through different types of investors, starting from the defensive investor who is someone a lot more careful to speculate. Majority of stock buybacks are done so to counteract the execution of employee options. The key to value investing is purchasing stocks that are selling well below their “intrinsic” value. This is the best book on investing. A definitive read for those looking for a disciplined approach to investment. Many claim that it is still entirely relevant, despite its age.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Deep Work: Rules for Focused Success in a Distracted World<br>\n",
    "**Generated Description:**<br>\n",
    "\"Deep Work\" is about why and how to manage to work deeply, i.e., producing precious value. To learn quickly, you need to study for long periods of time consistently. Social media, if used without limit, can be particularly devastating to your quest to work deeper. To conclude, I’d like to recommend ‘Deep Work’ to anyone seeking to develop a more productive work routine. Read David Allen instead, whose ideas permeate this book to a degree, but who cannot be quoted every second page.\n",
    "\n",
    "**Book Title:**<br>\n",
    "Mere Christianity<br>\n",
    "**Generated Description:**<br>\n",
    "\"I truly loved this book and recommend to all Christians ... even people who aren’t Christians will get a great grasp of what Christianity is all about\" \"I was alight with curiosity. The concepts and thoughts he introduced were really interesting, because what Lewis does is he takes normal, everyday concepts for a Christian and examines them to the point that you completely understand where and how and why they are\" \"His style of explaining things in such a blunt, straightforward manner was so relatable at times\"\n",
    "\n",
    "---\n",
    "Generating summaries from book reviews is a powerful approach to capturing the essence of a book through the perspectives of multiple readers. By condensing a range of opinions, experiences, and highlights from reviews, we can create a more balanced and comprehensive description that goes beyond a typical synopsis. This method of summary generation can be applied to many other forms of reviews (product, location, etc) and has been done by companies like Amazon for their product reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ff5c4-16d0-44db-9776-32baab367ad5",
   "metadata": {},
   "source": [
    "# Part 2: Manual Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8461ae-172c-4c6f-9be0-4550e3e35c53",
   "metadata": {},
   "source": [
    "## Star Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d7bf3-a606-4725-8df9-5e379a45aea7",
   "metadata": {},
   "source": [
    "Users give a book a _Star Rating_ along with leaving a _text review_ on a book, we investigate the question: \"Can a Neural Network be used to accurately predict the number of stars of a review based on the text?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a0b09d-a8f7-45cc-8080-d0a56cff5b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Review Text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b91c61-04e5-4d96-9f4e-be0300e6290c",
   "metadata": {},
   "source": [
    "Some reviews texts are empty, it is quite few. Therefore we can safely omit these reviews from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bdd511-af11-4fa8-b282-a282a2174033",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_noNA = reviews.dropna(subset=['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dca2ba-3bc6-4474-ad56-756e004e41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target and predictor datasets\n",
    "X = reviews_noNA['Review Text'].values\n",
    "y = reviews_noNA['Review Stars'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d2077-2278-458b-bb60-d61579b87348",
   "metadata": {},
   "source": [
    "We now use the `TfidfVectorizer` from `sklearn` to vectorize the text into term-frequency inverse-document frequency matrix form. This approach helps capture the importance of words across the dataset, where terms that occur frequently in a document but rarely across all documents are given higher weight, while common terms across the corpus are downweighted. The `LabelEncoder` converts categorical labels into numerical ones to feed into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6cab88f-a862-4d3c-bb2a-99d4d99592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75daf232-eaf2-4b1f-8b12-c901fe69cc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 19:46:48.459731: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5012 - loss: 1.2103 - val_accuracy: 0.5845 - val_loss: 0.9857\n",
      "Epoch 2/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7108 - loss: 0.7266 - val_accuracy: 0.6112 - val_loss: 0.9876\n",
      "Epoch 3/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8710 - loss: 0.3772 - val_accuracy: 0.6072 - val_loss: 1.2340\n",
      "Epoch 4/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.1082 - val_accuracy: 0.6062 - val_loss: 1.8203\n",
      "Epoch 5/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0303 - val_accuracy: 0.6000 - val_loss: 2.3857\n",
      "Epoch 6/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0127 - val_accuracy: 0.5986 - val_loss: 2.5610\n",
      "Epoch 7/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9965 - loss: 0.0124 - val_accuracy: 0.5989 - val_loss: 2.7659\n",
      "Epoch 8/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0109 - val_accuracy: 0.6085 - val_loss: 2.9141\n",
      "Epoch 9/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0118 - val_accuracy: 0.5901 - val_loss: 3.0036\n",
      "Epoch 10/10\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9976 - loss: 0.0079 - val_accuracy: 0.5963 - val_loss: 2.9819\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') # Softmax for categorical classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbde494c-a1f9-4ca8-8df2-a38f9fe5e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.603\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4355d99-9925-4f18-9dd7-b687ee5706e7",
   "metadata": {},
   "source": [
    "A test accuracy around 0.5 means we could have predicted 2.5 stars for every review and gotten the same accuracy. The training accuracy seems to increase drastically however validation accuracy remains steady at around 0.51. This indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc54f9-ee3b-409e-82c2-2c1704330955",
   "metadata": {},
   "source": [
    "What appears to be interesting is that despite the model statistically indicating overfitting, the predictions it makes can actually be quite reliable.\n",
    " Consider an example of a bad review. \"This book was absolutely terrible! How could you think this was a good idea.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d55bf8-6ee0-4b07-bcb3-cc4db89a9714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted rating: 1\n"
     ]
    }
   ],
   "source": [
    "# BAD review example\n",
    "sample_review = [\"This book was absolutely terrible! How could you think this was a good idea.\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf4f2e-9ae0-4953-856e-07ac75750833",
   "metadata": {},
   "source": [
    "Consider an example of a long mixed review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2ec3016-8439-414d-a1e6-7d3083457bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Predicted rating: 3\n"
     ]
    }
   ],
   "source": [
    "sample_review = [\"I had high hopes for The Infinite Horizon after hearing so much about it. From the beginning, the premise seemed promising, and for the most part, it delivers on its intriguing concept. The plot revolves around a futuristic world where society grapples with the boundaries of artificial intelligence, humanity, and survival—concepts that have always fascinated me. The world-building is impressive, with detailed landscapes and a unique societal structure that keeps you hooked initially. The author has clearly put a lot of thought into constructing the futuristic world, and it shows in the vivid descriptions and creative technologies. However, while the world-building is rich, the characters left much to be desired. The protagonist, Lila, felt underdeveloped. I found myself frustrated at several points because her motivations were either unclear or inconsistent. In the beginning, she starts off as a strong, determined character, but midway through, her actions seem erratic and her growth stagnates. The dialogue, too, felt stilted at times, making it hard to connect with the characters emotionally. There were a few moments where I felt the conversations between key characters were forced, almost like they were inserted to explain plot points rather than feeling organic.On the flip side, I have to give credit where it’s due—the pacing of the story is solid for the most part. There are intense moments where you’re on the edge of your seat, particularly during the battle scenes. These scenes were written with such vivid detail that I could easily imagine them playing out in a movie. The action sequences are well thought out, and they definitely add excitement to the narrative. That being said, there were also moments where the pacing lagged, especially in the middle sections. Some chapters felt like filler, dragging on with unnecessary exposition and side plots that didn’t add much to the overarching story.\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a201d-8fa7-4364-9b07-583dfc480d63",
   "metadata": {},
   "source": [
    "The rating prediction appears to be somewhat reliable even with longer mixed reviews. Now let's consider an example of a great review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b1445f-7652-4a9f-8d53-cb3f6179ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Predicted rating: 5\n"
     ]
    }
   ],
   "source": [
    "sample_review = [\"This book was a captivating read from start to finish. The characters felt incredibly real, and the plot twists kept me on the edge of my seat. I couldn't put it down and will definitely be recommending it to everyone!\"]\n",
    "sample_review_tfidf = vectorizer.transform(sample_review)\n",
    "prediction = model.predict(sample_review_tfidf.toarray())\n",
    "predicted_rating = np.argmax(prediction) + 1\n",
    "print(f'Predicted rating: {predicted_rating}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdca806-235b-4aa0-ab0b-27422cc4c9e7",
   "metadata": {},
   "source": [
    "#### Alternative models to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e336c8e2-7766-4c4b-853b-c7834e1d4d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4549 - loss: 1.3119 - val_accuracy: 0.5340 - val_loss: 1.1000\n",
      "Epoch 2/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5988 - loss: 0.9584 - val_accuracy: 0.5446 - val_loss: 1.1051\n",
      "Epoch 3/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6959 - loss: 0.7445 - val_accuracy: 0.5276 - val_loss: 1.2130\n",
      "Epoch 4/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7953 - loss: 0.5510 - val_accuracy: 0.5144 - val_loss: 1.4226\n",
      "Epoch 5/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8697 - loss: 0.3718 - val_accuracy: 0.4957 - val_loss: 1.7808\n",
      "Epoch 6/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.2650 - val_accuracy: 0.5031 - val_loss: 2.1641\n",
      "Epoch 7/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9228 - loss: 0.2165 - val_accuracy: 0.4934 - val_loss: 2.4140\n",
      "Epoch 8/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9436 - loss: 0.1696 - val_accuracy: 0.4984 - val_loss: 2.4654\n",
      "Epoch 9/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9539 - loss: 0.1442 - val_accuracy: 0.4932 - val_loss: 2.7137\n",
      "Epoch 10/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9556 - loss: 0.1309 - val_accuracy: 0.4973 - val_loss: 2.7589\n"
     ]
    }
   ],
   "source": [
    "# Alternative model incorporating Dropout\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "372999e7-4c7a-48ec-948a-7154e044fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.492\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042c812-4f84-464a-a53d-e0ca3381c91d",
   "metadata": {},
   "source": [
    "Adding dropout layers does not improve the statistical overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1524b67-e90e-424c-8cd1-0a6c7621cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use L2 Regularization to attempt to improve overfitting through weight decay\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81520d4f-1862-4d54-b3c8-f1772d04b429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4636 - loss: 1.9923 - val_accuracy: 0.4724 - val_loss: 1.3309\n",
      "Epoch 2/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4699 - loss: 1.3186 - val_accuracy: 0.4724 - val_loss: 1.3181\n",
      "Epoch 3/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4777 - loss: 1.3026 - val_accuracy: 0.4724 - val_loss: 1.3107\n",
      "Epoch 4/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4710 - loss: 1.3012 - val_accuracy: 0.4724 - val_loss: 1.3085\n",
      "Epoch 5/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4808 - loss: 1.2941 - val_accuracy: 0.4724 - val_loss: 1.3074\n",
      "Epoch 6/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4784 - loss: 1.2939 - val_accuracy: 0.4724 - val_loss: 1.3067\n",
      "Epoch 7/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4730 - loss: 1.3049 - val_accuracy: 0.4724 - val_loss: 1.3066\n",
      "Epoch 8/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4659 - loss: 1.3017 - val_accuracy: 0.4724 - val_loss: 1.3065\n",
      "Epoch 9/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4775 - loss: 1.3018 - val_accuracy: 0.4724 - val_loss: 1.3070\n",
      "Epoch 10/10\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4716 - loss: 1.3072 - val_accuracy: 0.4724 - val_loss: 1.3065\n",
      "Test accuracy: 0.484\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_test.toarray(), y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1530fdb-f3cc-4a36-ba60-70b7dd828616",
   "metadata": {},
   "source": [
    "When applying L2 regularization the result is the model tends to _underfit_. We see this because both the training and validation accuracies are low and nearly identical, meaning the model is not complex enough to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455b952-4e93-4212-9263-c1fc2f1c1b82",
   "metadata": {},
   "source": [
    "# Part 3: Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11b440-d094-4065-81ba-62d94fccef35",
   "metadata": {},
   "source": [
    "Books are often reviewed based on certain aspects, for example, a fiction book on the plot, characters or maybe the setting. Readers have diverse tastes and some may prioritise plot over characters or maybe emotion over humour. We therefore propose an Aspect-Based Sentiment Analysis (ABSA) of our book reviews. This can help identify strengths and can guide authors and publishers towards understanding the reader's perspective. By highlighting both positive and negative aspects, we can produce a balanced critique with a well-rounded perspective. This also enhances decision-making for readers who do not want to sift through hundreds of wordy reviews to gauge a sentiment on a specific aspect of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd156b3a-112e-48fb-b6f2-78b58f5066e8",
   "metadata": {},
   "source": [
    "## [3A] Simple Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30554d-64fa-466d-a497-22ded57f8313",
   "metadata": {},
   "source": [
    "A simple and interpretable approach to this problem is to:\n",
    "1. Pre-define certain aspects.\n",
    "2. Select sentences containing that aspect.\n",
    "3. Compute sentiment of those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7fab6abc-d846-483c-b4c7-f9671ddf27cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Define aspects we're interested in\n",
    "aspects = [\"plot\", \"characters\", \"writing\", \"pacing\", \"setting\", \"structure\", \"emotion\", \"humor\"]\n",
    "\n",
    "def preprocess_lemmatize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def extract_aspects(tokens):\n",
    "    extracted_aspects = []\n",
    "    for aspect in aspects:\n",
    "        if aspect in tokens:\n",
    "            extracted_aspects.append(aspect)\n",
    "    return extracted_aspects\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def extract_aspect_sentences(text, aspect):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [sent for sent in sentences if aspect in sent.lower()]\n",
    "\n",
    "def analyze_review(review):\n",
    "    tokens = preprocess_lemmatize(review)\n",
    "    extracted_aspects = extract_aspects(tokens)\n",
    "    results = {}\n",
    "    \n",
    "    for aspect in extracted_aspects:\n",
    "        aspect_sentences = extract_aspect_sentences(review, aspect)\n",
    "        if aspect_sentences:\n",
    "            sentiment = sum(analyze_sentiment(sent) for sent in aspect_sentences) / len(aspect_sentences)\n",
    "            results[aspect] = {\n",
    "                \"sentiment\": sentiment,\n",
    "                \"sentences\": aspect_sentences\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eff70144-c503-4fc0-a518-8fa3d643a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: plot\n",
      "Sentiment: 0.30\n",
      "Supporting sentences:\n",
      "- \n",
      "The book had an intriguing plot that kept me guessing until the end.\n",
      "\n",
      "Aspect: writing\n",
      "Sentiment: 0.12\n",
      "Supporting sentences:\n",
      "- The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
      "\n",
      "Aspect: pacing\n",
      "Sentiment: -0.15\n",
      "Supporting sentences:\n",
      "- However, the pacing was a bit slow in the middle sections.\n",
      "\n",
      "Aspect: setting\n",
      "Sentiment: 0.12\n",
      "Supporting sentences:\n",
      "- The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best case example\n",
    "example_review = \"\"\"\n",
    "The book had an intriguing plot that kept me guessing until the end. \n",
    "The characters were well-developed and relatable. \n",
    "However, the pacing was a bit slow in the middle sections. \n",
    "The writing style was eloquent, and the author's descriptions of the setting were vivid.\n",
    "\"\"\"\n",
    "\n",
    "results = analyze_review(example_review)\n",
    "\n",
    "for aspect, data in results.items():\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Sentiment: {data['sentiment']:.2f}\")\n",
    "    print(\"Supporting sentences:\")\n",
    "    for sentence in data['sentences']:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd4f6693-431f-4817-a814-34bee5dceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the correct subset\n",
    "born_a_crime_subset = reviews[reviews['Book Title'] == 'Born a Crime: Stories From a South African Childhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68837edd-0b94-40ec-b1f1-04e0c3e203a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'d rate this 4.5 stars. I was really surprised when Trevor Noah was named Jon Stewart\\'s successor on The Daily Show . I inherently knew that they wouldn\\'t pick someone with a sense of humor and style identical to Stewart\\'s, but I felt that Noah was so different that his selection meant the show would have a really different feel, which might not appeal to long-time fans of the show. But I always root for the underdog, so as he was getting savaged by critics and fans in his first few days on the job, I kept hoping he\\'d be able to tough it out and show the stuff—comedic and otherwise—of which he was made. After reading , I realize that I needn\\'t have worried about Trevor Noah. For a child growing up in South Africa in the last days of, and the tumult following apartheid, he faced crises far greater than dissatisfied fans. And if he could be raised during such a crazily illogical time in a country where more violence, racism, and mistreatment went unreported than caught the media\\'s eye, he\\'d have no problem skewering the insanity of our political system, especially leading into the election of 2016!! \"On February 20, 1984, my mother checked into Hillbrow Hospital for a scheduled C-section delivery. Estranged from her family, pregnant by a man she could not be seen with in public, she was alone. The doctors took her up to the delivery room, cut open her belly, and reached in and pulled out a half-white, half-black child who violated any number of laws, statutes, and regulations—I was born a crime.\" Born to a black Xhosa mother and a white Swiss father, Noah literally spent his earliest days hiding indoors. His parents, who never married, couldn\\'t be seen together, and because his mother looked so different than he did, she couldn\\'t walk through the streets with him, because at any moment someone might accuse her of kidnapping another person\\'s child. Yet while their lives dealt with crushing poverty, violence, and racism from all sides, his deeply religious mother never let anything bother her, or stop her from raising her son to know he was loved, and to know that he truly could accomplish anything he wanted, despite all of the obstacles in his way. \"She taught me to challenge authority and question the system. The only way it backfired on her was that I constantly challenged and questioned her.\" provides a first-hand account of the last days of apartheid and its aftermath, and what it was like to grow up as a mixed-race child, where he wasn\\'t white enough to be considered white, nor was he black enough to be considered black. While at times this had its advantages, for the most part, it left him on the outside looking in, having to handle everything on his own, fight his own battles, struggle to find people who genuinely liked him for who he was and not the novelty of his skin color, and rebel against a mother who only wanted him to behave. If you go into this book expecting to laugh hysterically because of Noah\\'s day job, think again. While the book does include some of the wry humor that has begun endearing him to fans, this is an emotional, brutal, and educational story of a life which flourished despite the odds stacked against it. This is a book about growing up in a culture of poverty and crime, and how easy it was to get caught up in that, especially when it was one of the only ways to make money and be able to feed, clothe, and enjoy yourself. It\\'s also a book about fear, how it motivates you, how it paralyzes you, and how it threatens to take away the one thing you cherish more than any other. More than anything, though, this is a book about the unwavering love of a mother for a child she chose to have. She knew it would be difficult raising her son in the age of apartheid, and in fact, she had no idea when he was born that it would end anytime soon. But Noah was a remarkable child, and while he exasperated, frightened, and upset his mother from time to time, she knew he would accomplish great things one day (as soon as he stopped putting cornrows in his hair and hanging out with those awful hoodlums he called friends). I enjoyed this book and learned a lot about apartheid, which I really didn\\'t know much about. Noah is a good writer, and delivered his narrative much as I\\'ve heard him deliver his lines on . This is a funny, thought-provoking, and emotional book, although I felt that some of his anecdotes went on a little too long, while others didn\\'t go on long enough. I also would have liked to have learned how he went from his upbringing in South Africa to one day hosting an acclaimed television show—other than passing mentions of things he did, I have no idea how he made the leap. I\\'ve heard some people say that the audio version of this book is brilliant because Noah reads it himself, but if you read the print/digital version, you can still hear his voice through his words. Noah\\'s story is a lesson of the inequities of the past, and a warning for what is still possible to happen again in our world. But this isn\\'t heavy-handed; it\\'s fun, insightful, and very compelling. See all of my reviews at , and see my list of the best books I read in 2016 at .'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "born_a_crime_subset['Review Text'][7679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f5fcd468-0e81-4882-b9ed-f24158bad81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: humor\n",
      "Sentiment: -0.02\n",
      "Supporting sentences:\n",
      "- I inherently knew that they wouldn't pick someone with a sense of humor and style identical to Stewart's, but I felt that Noah was so different that his selection meant the show would have a really different feel, which might not appeal to long-time fans of the show.\n",
      "- While the book does include some of the wry humor that has begun endearing him to fans, this is an emotional, brutal, and educational story of a life which flourished despite the odds stacked against it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "born_a_crime_subset['Review Text'][7679]\n",
    "results = analyze_review(born_a_crime_subset['Review Text'][7679])\n",
    "\n",
    "for aspect, data in results.items():\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Sentiment: {data['sentiment']:.2f}\")\n",
    "    print(\"Supporting sentences:\")\n",
    "    for sentence in data['sentences']:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c7ff1-6bef-4587-b255-9014548db923",
   "metadata": {},
   "source": [
    "## [3B] Advanced Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95a440c4-0a45-44b1-90f3-827f02720386",
   "metadata": {},
   "source": [
    "The ABSA field is an extremely active area of research. A popular dataset which is often used as a benchmark for ABSA models is the [SemEval 2014 Task 4](https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval). We considered many alternative papers and datasets, and finally settled on [InstructABSA](https://arxiv.org/abs/2302.08624v6) due to its incredible performance on benchmarks and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73220cb-4421-48bd-999b-383c254d8638",
   "metadata": {},
   "source": [
    "### InstructABSA Architecture\n",
    "This model introduces positive, negative, and neutral examples to each training sample, and instruction tune the model [Tk-Instruct](https://aclanthology.org/2022.emnlp-main.340/) for ABSA subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b57336-dffa-4492-a842-0fcecfaff55f",
   "metadata": {},
   "source": [
    "The ABSA subtasks can be represented as follows: Let $S_i$ represent the $i^{th}$ review sentence in the training sample, where $S_i = {w_{i}^1, w_{i}^2, ..., w_{i}^n}$ with $n$ as the number of tokens in the sentence. \n",
    "Each $S_i$ contains a set of aspect terms denoted by $A_i = {a_{i}^1, a_{i}^2, ..., a_{i}^m} | m \\le n$, and the corresponding opinion terms and sentiment polarities for each aspect term are denoted by $O_{i} = {o_{i}^1, o_{i}^2, ..., o_{i}^m}$ and $SP_{i} = {sp_{i}^1, sp_{i}^2, ..., sp_{i}^m}$ respectively, where $sp_i^k \\in [ positive, negative, neutral ]$. \n",
    "\\\n",
    "\\\n",
    "The ABSA tasks are described as follows:\\\n",
    "ATE: $A_i = LM_{ATE}(S_i)$\\\n",
    "ATSC: $sp_i^k = LM_{ATSC}(S_i, a_i^k)$\\\n",
    "ASPE: $[A_i, SP_i] = LM_{ASPE}(S_i)$\\\n",
    "AOOE: $o_{i}^k = LM_{AOOE}(S_i, a_i^k)$\\\n",
    "AOPE: $[A_i, O_i] = LM_{AOPE}(S_i)$\\\n",
    "AOSTE: $[A_i, O_i, SP_i] = LM_{AOSTE}(S_i)$\\\n",
    "\\\n",
    "In these equations, $LM$ represents the language model, and the corresponding inputs and outputs are defined accordingly. InstructABSA, in their approach, instruction tuned $LM_{subtask}$ by prepending task-specific prompts to each input sample to arrive at $LM_{subtask}^{Inst}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8f9ae-1d28-4279-b8b6-f47dac9c4757",
   "metadata": {},
   "source": [
    "We specifically consider ATSC for our use case of identify the sentiment around a specific aspect of a book review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d6135-78f3-46d0-a3b6-71ea7a1800f4",
   "metadata": {},
   "source": [
    "![ABSASubtasks.png](images/ABSASubtasks.png)\n",
    "\n",
    "Image Source: [InstructABSA](https://arxiv.org/abs/2302.08624v6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6ee53-d216-4cc6-a5a2-94bcc377c524",
   "metadata": {},
   "source": [
    "![overview.png](images/overview.png)\n",
    "Image Source: [InstructABSA](https://arxiv.org/abs/2302.08624v6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a94eb1-b978-4334-956e-56a35ef6a8ab",
   "metadata": {},
   "source": [
    "### Tk-INSTRUCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99663bd6-285d-461a-a9c1-280e44dd3ee6",
   "metadata": {},
   "source": [
    "Tk-INSTRUCT is a transformer model trained to follow a variety of in-context instructions (plain language task definitions or _k_-shot examples). It builds on the T5 text-to-text transformer model using an instruction tuning approach. It converts diverse NLP tasks into a consistent instruction format through a task format:\n",
    "\n",
    "- **Definition:** Task description\n",
    "- **Things to avoid:** Common mistakes\n",
    "- **Positive examples:** Good completions\n",
    "- **Negative examples:** Poor completions\n",
    "- **Input** \n",
    "- **Output**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eea556-1039-49a8-94a1-8cee57995750",
   "metadata": {},
   "source": [
    "### Understanding the T5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0436d-2381-4f66-9b0f-f26b1f61225e",
   "metadata": {},
   "source": [
    "![ABSA.png](images/ABSA.png)\n",
    "Part of this image was adapted from: [Jay Alammar’s blog](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5f437-4dcd-49e1-bf74-7cf0af1673fd",
   "metadata": {},
   "source": [
    "At the bottom of the hierarchy of what we use lies T5, which stands for  _Text-To-Text Transfer Transformer_ proposed by [Google in 2020](https://arxiv.org/abs/1910.10683). It was trained on a cleaned common crawl web extracted [text corpus](https://www.tensorflow.org/datasets/catalog/c4). It is based on a BERT-sized encoder-decoder transformer which is illustrated in the image above. Since the dataset is unlabelled, an unsupervised objective was selected to allow learning from the unlabelled data. Words are dropped out independently and uniformly at random and replaced with a unique sentinel token. The model is then trained to predict sentinal tokens to delineate the dropped-out text (refer to image below). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b98eaa-f0f1-4fa3-bec9-22b955e95dcb",
   "metadata": {},
   "source": [
    "![T5objective.png](images/T5objective.png)\n",
    "Image Source: [T5 Paper](https://arxiv.org/abs/1910.10683) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3037021-c663-45ee-921c-d4858af5ec9a",
   "metadata": {},
   "source": [
    "To focus in on the core structure of the T5 transformer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f86343-0908-4aa2-8cc1-05a53e048b69",
   "metadata": {},
   "source": [
    "**Input Representation**\n",
    "\n",
    "1. Tokenization:\n",
    "\n",
    "    Uses `SentencePiece`, which creates a vocabulary of subword units from an input text. For example the instruction \"Summarize this review: This book rules!\" could be tokenized into [\"summarize\", \"_this\", \"_review\", \":\", \"_This\", \"_book\", \"_rules\", \"!\"].\n",
    "\n",
    "3. Conversion to Token IDs:\n",
    "\n",
    "    Every token is mapped to a unique integer ID from the vocabulary.\n",
    "\n",
    "4. Embedding:\n",
    "\n",
    "   The token IDs are then converted into dense vector embeddings, along with positional embeddings to encode the token's position in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eceee0e-657a-4d9a-a6bd-3d3a1f9d37eb",
   "metadata": {},
   "source": [
    "\n",
    "**Encoder**\n",
    "\n",
    "The encoder stage is built with multiple layers (12 layers in T5-Base, the smallest T5 model). Each layer consists of:\n",
    "\n",
    "1. Self-Attention Mechanism:\n",
    "\n",
    "   This transforms the input text into 3 vectors through linear transformations of the input: _query_ ($Q$), _key_ ($K$) and _value_ ($V$). The attention mechanism then calculates a weighted sum of generated values based on the similarity between query and key vectors. It essentially takes into consideration the relationship among words within the same sentence. With self-attention, we are feeding the **same embedding into all 3 layers**. The attention formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08529e-a187-4278-b1a5-0b71efd85890",
   "metadata": {},
   "source": [
    "![SelfAttention.png](images/SelfAttention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5892de-2377-498c-b00f-2715df5404ba",
   "metadata": {},
   "source": [
    "3. Feed-Forward Neural Network:\n",
    "\n",
    "    2-layered fully connected network using the ReLU activation.\n",
    "\n",
    "4. Layer Normalization:\n",
    "\n",
    "    Applied before each sub-layer to stabilize training (pre-norm), this makes the model more robust to learning rate changes. T5 does not use scaling (gamma) and bias (beta) parameters.\n",
    "\n",
    "5. Residual Connections:\n",
    "\n",
    "   Skip-connections are also included around each sub-layer to improve gradient flow during training. This ensures the model does not rely on certain weights too heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a549a0-d737-4bcc-a397-c359fe685b79",
   "metadata": {},
   "source": [
    "**Decoder**\n",
    "\n",
    "The decoder also consists of multiple layers (12 layers in T5-Base). Each layer includes:\n",
    "\n",
    "1. Self-Attention Mechanism:\n",
    " \n",
    "    The implementation is similar to the encoder. The addition of masking ensures that positions do not attend to future positions in the output sequence.\n",
    "\n",
    "3. Cross-Attention Mechanism (Encoder-Decoder Attention):\n",
    "\n",
    "    This enables each position in the decoder to attend to all positions in the encoder’s output.\n",
    "\n",
    "4. Feed-Forward Neural Network:\n",
    "\n",
    "    Same concept as the encoder.\n",
    "\n",
    "5. Layer Normalization:\n",
    "\n",
    "    Applied before each sub-layer (pre-norm), similarly to the encoder.\n",
    "\n",
    "6. Residual Connections:\n",
    "\n",
    "    Same as the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714269b4-45c5-4e91-9de1-ce6e3e97fe61",
   "metadata": {},
   "source": [
    "### Applying InstructABSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae55e1c-d8a2-4469-b5b9-a25659a48cd5",
   "metadata": {},
   "source": [
    "We begin by preloading InstructABSA from [HuggingFace](https://huggingface.co/kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined). With that we predefine certain prompts for the aspect-based sentiment analysis. This primes InstructABSA for our task. We continue to pass in the reviews from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f47cba7e-29d7-4ff5-9b57-da9d421940c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c0d88906-9962-463b-a084-ac6434c5ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined\")\n",
    "\n",
    "# Predefine prompt for aspect based sentiment analysis\n",
    "bos_instruction = \"\"\"Definition: The output will be the aspects and their sentiment polarity for book reviews. The aspects to be considered are exclusively: plot, characters, writing, pacing, setting, structure, emotion, and humor. In cases where none of these aspects are mentioned, the output should be noaspectterm:none. The sentiment can be positive, negative, or neutral.\n",
    "Positive examples:\n",
    "Input: The characters were so well-developed, I felt like I knew them personally by the end of the book.\n",
    "Output: characters:positive\n",
    "Input: The author's writing style was captivating, and the plot kept me guessing until the very end.\n",
    "Output: writing:positive, plot:positive\n",
    "Input: The pacing was perfect, with just the right balance of action and reflection, and the Victorian London setting was vividly described.\n",
    "Output: pacing:positive, setting:positive\n",
    "Negative examples:\n",
    "Input: The plot had too many holes and inconsistencies to be enjoyable.\n",
    "Output: plot:negative\n",
    "Input: While the concept was interesting, the writing felt amateurish and the characters were one-dimensional.\n",
    "Output: writing:negative, characters:negative\n",
    "Input: The book's structure was confusing, jumping between timelines without any clear purpose.\n",
    "Output: structure:negative\n",
    "Neutral examples:\n",
    "Input: The setting was a typical fantasy world, neither particularly innovative nor disappointing.\n",
    "Output: setting:neutral\n",
    "Input: The author attempted to inject humor into the story, but it didn't particularly stand out.\n",
    "Output: humor:neutral\n",
    "Input: The emotional content of the book was present but didn't significantly impact my reading experience.\n",
    "Output: emotion:neutral\n",
    "Mixed sentiment example:\n",
    "Input: The plot was gripping, but the pacing felt off, with some sections dragging on too long.\n",
    "Output: plot:positive, pacing:negative\n",
    "No relevant aspect example:\n",
    "Input: The book arrived on time and in good condition.\n",
    "Output: noaspectterm:none\n",
    "Now complete the following example-\n",
    "Input:\"\"\"\n",
    "delim_instruct = ' The aspect is '\n",
    "eos_instruct = '.\\noutput:'\n",
    "\n",
    "def book_absa(book_title):\n",
    "    book_subset = reviews[reviews['Book Title'] == book_title]\n",
    "    book_reviews = book_subset['Review Text'].tolist()\n",
    "    book_summaries = summarize(book_reviews)\n",
    "    \n",
    "    book_final_summary = summarize_final(book_summaries)\n",
    "    \n",
    "    text = f'''{book_final_summary}'''\n",
    "    aspect_sentiment_dict = {}\n",
    "    \n",
    "    for aspect_term in aspects:\n",
    "        tokenized_text = tokenizer(bos_instruction + text + delim_instruct + aspect_term + eos_instruct, return_tensors=\"pt\")\n",
    "        output = model.generate(tokenized_text.input_ids)\n",
    "        sentiment_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Store the aspect term and its corresponding sentiment in the dictionary\n",
    "        aspect_sentiment_dict[aspect_term] = sentiment_output\n",
    "        print(f'Model output for {aspect_term}: ', sentiment_output)\n",
    "    \n",
    "    return aspect_sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c352f",
   "metadata": {},
   "source": [
    "#### Print the ABSA for the highest rated book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2df5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book with the highest average rating: Night (Average Rating: 4.762711864406779)\n"
     ]
    }
   ],
   "source": [
    "reviews['Review Stars'] = pd.to_numeric(reviews['Review Stars'], errors='coerce')\n",
    "\n",
    "# Group by 'Book Title' and calculate the average rating\n",
    "average_ratings = reviews.groupby('Book Title')['Review Stars'].mean()\n",
    "\n",
    "# Find the book with the highest average rating\n",
    "highest_rated_book = average_ratings.idxmax()\n",
    "highest_average_rating = average_ratings.max()\n",
    "\n",
    "print(f'Book with the highest average rating: {highest_rated_book} (Average Rating: {highest_average_rating})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a681239-9e86-4770-89c4-4296f248efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Number of reviews: 118\n",
      "Splitting into sentences...\n",
      "Starting to encode...\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21571abd4c64a56a55df034bb7478ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Finished\n",
      "Clustering Finished\n",
      "Loading pre-trained Sentence-BERT model...\n",
      "Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748e07ff7e464666b9a55a02a721ca23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences...\n",
      "Final summary generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User-PC\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output for plot:  positive\n",
      "Model output for characters:  positive\n",
      "Model output for writing:  positive\n",
      "Model output for pacing:  positive\n",
      "Model output for setting:  neutral\n",
      "Model output for structure:  conflict\n",
      "Model output for emotion:  positive\n",
      "Model output for humor:  positive\n"
     ]
    }
   ],
   "source": [
    "absa_sentiments = book_absa('Night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6affbf8f-d63b-4fca-9d0d-40cafb8e4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plot': 'positive', 'characters': 'positive', 'writing': 'positive', 'pacing': 'positive', 'setting': 'neutral', 'structure': 'conflict', 'emotion': 'positive', 'humor': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print(absa_sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
