---
title: "Goodreads Reviews Analysis"
author: "David Nicolay 26296918, Kellen Mossner 26024284, Matthew Holm 26067404"
date: "2024-10-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries and Custom Theme

```{r load libraries, echo=FALSE}
library(tidyverse)
library(tidytext)
library(stringr)
library(textdata)
library(lubridate)
library(ggplot2)
library(showtext)
library(ggridges)
library(wordcloud2)
library(plotly)
library(knitr)
library(cld2)
library(pluralize)
```

Setting up custom themes

```{r custom theme}
darjeeling1 <- c("#B22222", "#00A08A", "#F2AD00", "#F98400", "#5BBCD6")
axis_color <- "#2D2D2D"

# Set the font for plots
font_add_google("Montserrat") # Formal font
showtext_auto()

my_custom_theme <- theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#F3F1DE", color = NA),
    panel.background = element_rect(fill = "#F3F1DE", color = NA),
    title = element_text(color = "#2D2D2D", size = 20, family = "Montserrat"),
    axis.text = element_text(color = "#2D2D2D", size = 12, family = "Montserrat"),
    axis.title = element_text(face = "bold", color = "#FF8500", size = 14, family = "Montserrat"),
    legend.background = element_rect(fill = "#F3F1DE", color = NA),
    legend.key = element_rect(fill = "#F3F1DE", color = NA),
    panel.grid.major = element_line(color = "#2D2D2D", linewidth = 0.2),  # black gridlines
    panel.grid.minor = element_line(color = "#2D2D2D", linewidth = 0.1),
    axis.title.x = element_text(margin = margin(t = 15)),  # Add spacing to the top of the x-axis title
    axis.title.y = element_text(margin = margin(r = 15))  # Add spacing to the right of the y-axis title
  )

# Set as default theme
theme_set(my_custom_theme)
```

## Import the data

We import the Goodreads reviews dataset and the book genres dataset. We also detect the language of the reviews and filter the dataset to keep only English reviews. Finally, we join the reviews dataset with the book genres dataset to get the genres for each book.

```{r import}
reviews <- read.csv("data/goodreads_reviews_all.csv", header = TRUE)
book_genres <- read.csv("data/book_info.csv")

reviews$Review.Text <- tolower(reviews$Review.Text)
reviews$language <- detect_language(reviews$Review.Text)

# Filter the dataset to keep only English reviews
reviews <- reviews %>%
  filter(language == "en") %>%
  select(-language)
genres_list <- unlist(strsplit(book_genres$Genres, ", "))
unique_genres <- unique(genres_list)

# Create a new data frame with the books genres
reviews <- reviews %>%
  inner_join(book_genres, by = c("Book.Title" = "Book.Title"), relationship ="many-to-many")  # Join reviews with book genres
```

## Data Preprocessing

We added a review id column to our dataset, removed rows with missing values, removed duplicate reviews, converted the review date and the first published date to a date format, and removed special characters and converted the review text to lowercase.

```{r preprocessing}
# Add Review.ID
reviews <- reviews %>%
  mutate(Review.ID = row_number()) %>%         # Add the Review.ID column
  select(Review.ID, everything())

# Remove rows with missing values
reviews <- reviews %>%
  filter(!is.na(Review.Text))

# Remove duplicate reviews
reviews <- reviews %>%
  distinct(Review.ID, Book.Title, Link, Review.Date, Review.Text, Review.Stars, Review.Likes, Genres, First.Published.Date, Author)

# Convert Review.Date to Date format
reviews$Review.Date <- as.Date(reviews$Review.Date, format = "%B %d, %Y")
reviews$First.Published.Date <- as.Date(reviews$First.Published.Date, format = "%B %d, %Y")

# Remove all special characters and convert to lowercase
reviews$Review.Text <- reviews$Review.Text %>%
    str_replace_all("[^[:alnum:]\\s]", "") %>%   # Remove special characters
    str_replace_all("\\s+", " ") %>%            # Replace multiple spaces/newlines/tabs with a single space
    str_trim() %>%        # Trim leading and trailing whitespace
    tolower()             # Convert to lowercase
kable(head(reviews %>% 
  mutate(Review.Text = str_trunc(Review.Text, 100, "right"))),
  align = "l",  # left-align text
  caption = "Sample of Reviews")
```

## Exploratory Data Analysis

```{r dist of review stars}
ggplot(reviews, aes(x = Review.Stars)) +
  geom_histogram(binwidth = 1, fill = darjeeling1[2], color = "black", alpha = 0.8) +  # Set fill and outline colors
  scale_x_continuous(breaks = 1:5) +  # Set x-axis breaks to 1-5
  labs(title = "Distribution of Review Stars", x = "Review Stars", y = "Frequency")
```

We can see that the distribution of review stars is left skewed towards higher ratings, with a peak at 5 stars. This is a common pattern in review data, where people are more likely to leave reviews when they have strong positive or negative feelings about a product.<nl>

```{r review length}
reviews$Review.Length <- nchar(reviews$Review.Text)
avg_review_length <- reviews %>%
  group_by(Review.Stars) %>%
  summarise(Average.Length = mean(Review.Length))

# Plot the distribution of the average review length per star
ggplot(avg_review_length, aes(x = Review.Stars, y = Average.Length)) +
  geom_bar(stat = "identity", fill = darjeeling1[2], color="black", alpha=0.8) +
  labs(x = "Review Stars", y = "Average Review Length (characters)", 
       title = "Average Review Lengths per Star") 
```

We see that on average the reviews with lower ratings tend to be longer than those with higher ratings. This could be due to the fact that people are more likely to leave detailed reviews when they are dissatisfied with a book.

The most common words used in reviews, excluding the stop words and the words "book", "books" and "read".

```{r wordcloud}
# Tokenize the review text into words
word_counts <- reviews %>%
  unnest_tokens(word, Review.Text) %>%  # Tokenize review text into words
  count(word, sort = TRUE) %>%          # Count occurrences of each word
  filter(!word %in% stop_words$word) %>%   # Remove stop words
  filter(!word %in% 'book', !word %in% 'books', !word %in% 'read')

# Create a word cloud using wordcloud2
wordcloud2(data = word_counts, size = 1, minSize = 1, shape = 'circle')
```

Taking a look at the distribution of review stars for the top and worst rated books.

```{r dist of stars for top 10 books}
# Calculate average ratings and get the top and worst book
top_and_worst_books <- reviews %>%
  group_by(Book.Title) %>%
  summarise(Average.Rating = mean(Review.Stars, na.rm = TRUE)) %>%
  arrange(Average.Rating) %>%
  slice(c(n(), 1))  # Select the top (last) and worst (first) book, reversing their order

# Filter reviews for the top and worst books
top_worst_reviews <- reviews %>%
  filter(Book.Title %in% top_and_worst_books$Book.Title)

# Create a separate distribution plot for each book
ggplot(top_worst_reviews, aes(x = Review.Stars, fill = Book.Title)) +
  geom_histogram(alpha = 0.8, color = "black", binwidth = 1) +  # Create bar plots for the distribution of stars
  facet_wrap(~ Book.Title, ncol = 2) +  # Create two separate plots side by side
  scale_fill_manual(values = darjeeling1[c(1, 2)]) +
  labs(
       x = "Review Stars",
       y = "Number of Reviews") +
  theme(legend.position = "none",   # Remove the legend
        plot.title = element_text(hjust = 0.5, size = 14),  # Center and adjust title size
        plot.margin = margin(t = 20, r = 10, b = 10, l = 10))  # Add margin space around the plot
```

Although the reviews for The Holy Bible: King James Version are mostly 1-star reviews, the average rating for the book on goodreads.com is 4.44. We filtered reviews for each book by most popular reviews so it is unclear why most of the popular reviews are 1-star reviews. This could likely be due to an error on the goodreads algorithm that determines whether a review is popular or not.

```{r avg stars per year}
# Calculate average stars per year
avg_stars_per_year <- reviews %>%
  mutate(year = year(Review.Date)) %>%         # Extract the year from Review.Date
  group_by(year) %>%                           # Group by year
  summarize(avg_stars = mean(Review.Stars, na.rm = TRUE))  # Calculate average stars per year

# Plot both bar plot and line plot for average stars per year
ggplot(avg_stars_per_year, aes(x = year, y = avg_stars)) +
  geom_col(fill = darjeeling1[1], alpha = 0.7) +  # Bar plot with fill color and some transparency
  labs(title = "Average Stars per Year",
       x = "Year",
       y = "Average Stars") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Sentiment Analysis

We used the following 3 lexicons to calculate the sentiment score for each review: AFINN, Bing, and NRC.

```{r lexicons}
get_sentiments("afinn") %>% head(10)
get_sentiments("bing") %>% head(10)
get_sentiments("nrc") %>% head(10)
```

The top ten books with the highest total sentiment score across all reviews, as well as the bottom ten books with the lowest total sentiment score across all reviews.

```{r afinn sentiments}
reviews_sentiment <- reviews %>%
  unnest_tokens(word, Review.Text) %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(Book.Title) %>%
  summarize(sentiment = sum(value), avg_rating = mean(Review.Stars)) %>%
  arrange(desc(sentiment))

# View the top 10 books by sentiment score
head(reviews_sentiment, 10)
reviews_sentiment %>% arrange(sentiment) %>% head(10)
```

The book Night has the lowest total sentiment score across all reviews yet it also has the highest average rating across all reviews.

#### Genres for top and worst rated books

```{r genres}
# Join the reviews_sentiment dataset with the books_genres dataset
sentiment_with_genres <- reviews_sentiment %>%
  left_join(book_genres, by = "Book.Title")

# Get the top 10 highest sentiment books
top_highest_sentiment <- sentiment_with_genres %>%
  arrange(desc(sentiment)) %>%
  head(10)

# Get the top 10 lowest sentiment books
top_lowest_sentiment <- sentiment_with_genres %>%
  arrange(sentiment) %>%
  head(10)

# Print the results
print("Top 10 Highest Sentiment Books with Genres:")
print(top_highest_sentiment %>%
  select(Book.Title, Genres))

print("Top 10 Lowest Sentiment Books with Genres:")
print(top_lowest_sentiment %>%
  select(Book.Title, Genres))

```

Upon investigating the genres for the book Night, we see that the book is a historical biography about the holocaust which explains the low sentiment score for all the reviews. It is evident that the sentiment score is not a good indicator of the overall rating of a book. This is because the sentiment score is based on the sentiment of the words used in the reviews, which may not always reflect the overall sentiment of the review. For example, a review may contain positive sentiment words but still give a low rating to the book, or vice versa.

### Sentiment Analysis using NRC Lexicon

```{r nrc sentiments}
extended_palette <- colorRampPalette(darjeeling1)

# Tokenize the review text into words and calculate sentiment counts
sentiment_counts <- reviews %>%
  unnest_tokens(word, Review.Text) %>%          # Tokenize review text into words
  inner_join(get_sentiments("nrc"), by = "word", relationship = "many-to-many") %>% # Join with NRC lexicon
  count(sentiment) %>%                          # Count occurrences of each sentiment
  arrange(desc(n))

# Reorder sentiment factor levels by count
sentiment_counts$sentiment <- factor(sentiment_counts$sentiment, levels = sentiment_counts$sentiment)

# Get the number of unique sentiments
num_sentiments <- length(unique(sentiment_counts$sentiment))

ggplot(sentiment_counts, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_bar(stat = "identity", width = 0.9) +
  labs( x = "Sentiment", y = "Count") +
  scale_fill_manual(values = extended_palette(num_sentiments)) +  # Extend the palette to match sentiments
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1)) + # Customize axis titles
  scale_y_continuous(labels = scales::comma)
```

### Sentiment Analysis using Bing Lexicon

```{r bing sentiments}
bing_word_counts <- reviews %>%
  unnest_tokens(word, Review.Text) %>%
  inner_join(get_sentiments("bing"), by = join_by(word),relationship = "many-to-many") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  ggplot(aes(n, reorder(word, n), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment", y = NULL,title = "Top words contributing to sentiment") +
  scale_fill_manual(values = c("positive" = darjeeling1[5],   # Set a color for positive sentiment
                               "negative" = darjeeling1[1])) +  # Set a color for negative sentiment
  theme(strip.text = element_text(size = 14))
```

### Question: How does the sentiment of reviews correlate with the star rating given?

```{r}
# Calculate sentiment score for each review
sentiment_scores <- reviews %>%
  unnest_tokens(word, Review.Text) %>%
  inner_join(get_sentiments("afinn"), by = join_by(word)) %>%
  group_by(Book.Title, Link, Review.Date, Review.Stars, Review.Likes, Genres) %>%
  summarize(sentiment_score = sum(value), .groups = 'drop')

# Calculate correlation
correlation <- cor(sentiment_scores$Review.Stars, sentiment_scores$sentiment_score)
print(paste("Correlation between sentiment score and review stars:", correlation))
```

The correlation between sentiment score and review stars is 0.103, indicating a weak positive correlation between the two variables. This suggests that reviews with higher sentiment scores tend to have higher star ratings, but the relationship is not very strong.

```{r}
# Average sentiment score by star rating
avg_sentiment_by_stars <- sentiment_scores %>%
  group_by(Review.Stars) %>%
  summarize(avg_sentiment = mean(sentiment_score))

ggplot(avg_sentiment_by_stars, aes(x = Review.Stars, y = avg_sentiment)) +
  geom_col(fill = darjeeling1) +
  labs(title = "Average Sentiment Score by Star Rating",
       x = "Star Rating",
       y = "Average Sentiment Score") +
  theme(plot.title = element_text(hjust = 0.5))
```

The plot above shows the average sentiment score for each star rating. We can see that reviews with higher star ratings tend to have higher average sentiment scores, which is consistent with the weak positive correlation between sentiment score and review stars.

```{r avg sent per year bar}
avg_sentiment_per_year <- sentiment_scores %>%
  mutate(year = year(Review.Date)) %>%         # Extract the year from Review.Date
  group_by(year) %>%                           # Group by year
  summarize(avg_sentiment_score = mean(sentiment_score, na.rm = TRUE))  # Calculate average sentiment score

# Plot the average sentiment score per year as a histogram
ggplot(avg_sentiment_per_year, aes(x = year, y = avg_sentiment_score)) +
  geom_col(fill = darjeeling1[2], alpha = 0.7) +  # Use geom_col to create a histogram
  geom_smooth(method = "loess", se = FALSE, color = darjeeling1[1]) +
  labs(title = "Average Sentiment Score per Year",
       x = "Year",
       y = "Average Sentiment Score") +
  theme(plot.title = element_text(hjust = 0.5))
```

The plot above shows the average sentiment score per year. We can see that the average sentiment score has been relatively stable over the years, with noticeable dips in 2014 and then a steady decrease from 2018 to 2021 which could be related to the covid-19 pandemic.

## NLP Question: Which review words resulted in highest rating?

First let's clean the dataset for our specific purpose. The words should also be tokenized and stopwords removed.

```{r tokenize}
replace_reg <- "(https?:.*?([\\s]|[a-zA-Z0-9]$))|(www:.*?([\\s]|[a-zA-Z0-9]$))"
unnest_reg <- "[^A-Za-z_\\d#@']"
# Select the relevant columns
words_rating_reviews <- reviews %>% 
  select(Review.Text, Review.Stars)
  
# Tokenize the review text
words_ratings <- words_rating_reviews %>%
  unnest_tokens(word, Review.Text, token = "regex", pattern = unnest_reg, drop = FALSE) %>%
  anti_join(stop_words, by = join_by(word)) # Remove stop words
```

```{r calc_avg_rating}
# Calculate average ratings for each word
word_avg_ratings <- words_ratings %>%
  group_by(word) %>%
  summarise(
    avg_rating = mean(Review.Stars),
    count = n()
  ) %>%
  filter(count >= 10)  # Filter words that appear at least 10 times
```

```{r}
# Find the words with the highest ratings
top_words <- word_avg_ratings %>%
  arrange(desc(avg_rating)) %>%
  head(20)

# Display the results
print(top_words)
```

```{r}
# Find the words with the lowest ratings
bottom_words <- word_avg_ratings %>%
  arrange(avg_rating) %>%
  head(20)

# Display the results
print(bottom_words)
```
Apart from the irrelevant words such as author names etc., the positive words that stand out are: "bellowed", "anythings", "avengers", "blastended". The negative words that stand out are: "causation", "correlation", "unoriginal", "imply", "mindnumbingly".
As somewhat is to be expected, the common words that result in high reviews don't generalize well across books. No trend can easily be identified from this top average rating list.

## Topic Analysis Question: What are the most common topics or themes discussed in positive vs. negative reviews?

```{r}
unordered_books_sentiment <- sentiment_scores %>%
  group_by(Book.Title) %>%
  summarize(avg_sentiment = mean(sentiment_score),
            avg_stars = mean(Review.Stars),
            review_count = n())
head(unordered_books_sentiment)
```

```{r}
# Join the sentiment scores with the reviews
top_book_reviews <- unordered_books_sentiment %>%
  inner_join(reviews, by = "Book.Title") %>%
  select(Book.Title, Review.Text, Review.Stars, avg_sentiment) 

top_book_reviews <- top_book_reviews %>%
  mutate(review_type = case_when(
    Review.Stars >= 4 ~ "Positive",
    Review.Stars <= 2 ~ "Negative",
    TRUE ~ "Neutral"
  ))

# Tokenize the review text
tokenized_reviews <- top_book_reviews %>%
  unnest_tokens(word, Review.Text)

# Remove stop words
data("stop_words")
tokenized_reviews <- tokenized_reviews %>%
  anti_join(stop_words, by = "word")

# Count the most frequent words in positive and negative reviews
word_count <- tokenized_reviews %>%
  filter(review_type != "Neutral") %>%
  group_by(review_type, word) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Top words in positive reviews
top_positive_words <- word_count %>%
  filter(review_type == "Positive") %>%
  arrange(desc(count)) %>%
  head(10)

# Top words in negative reviews
top_negative_words <- word_count %>%
  filter(review_type == "Negative") %>%
  arrange(desc(count)) %>%
  head(10)

# View results
top_positive_words
top_negative_words
```

From this, we can see a lot of overlap (which makes sense in most cases since certain words are very likely to be used in book reviews in general - i.e. book, read, people, time). It would seem helpful to exclude these overlapping words when trying to distinguish unique themes or topics, but this would shrink the word counts (particularly for negative reviews) as shown below.

```{r}
# Step 1: Identify overlapping words
cleaned_word_count <- word_count %>%
  filter(str_detect(word, "^[a-zA-Z]+$"))  %>%   # Keep only English alphabetic words
  filter(!str_detect(word, "\\d"))  


overlap_words <- intersect(
  cleaned_word_count %>% filter(review_type == "Positive") %>% pull(word),
  cleaned_word_count %>% filter(review_type == "Negative") %>% pull(word)
)

# Step 2: Exclude overlapping words
filtered_word_count <- cleaned_word_count %>%
  filter(!word %in% overlap_words)

# Step 3: Get the top unique words for positive reviews
top_positive_unique_words <- filtered_word_count %>%
  filter(review_type == "Positive") %>%
  arrange(desc(count)) %>%
  head(10)
# Step 4: Get the top unique words for negative reviews
top_negative_unique_words <- filtered_word_count %>%
  filter(review_type == "Negative") %>%
  arrange(desc(count)) %>%
  head(10)

# View the results
top_positive_unique_words
top_negative_unique_words
```
It is important to note that our corpus of reviews consists of multiple harry potter books and thus removing the overlapping words will result in a significant reduction in the number of common words used in reviews adn the next most common words will become the words in the reviews that we have the most of - which are the Harry Potter books.

For positive reviews, common words like "Azkaban", "Douglass", and "Lockhart" reflect references to notable Harry Potter characters and themes. Words like "Enchanting" (142 mentions) and "Triwizard" (132 mentions) indicate a focus on the magical, thrilling elements of the stories. The high frequency of terms such as "Dementors" and "Hermione" further reinforces this idea of readers praising beloved characters and exciting events in the series.

On the other hand, negative reviews introduce terms that criticize certain aspects of the books or other elements. Words like "Causation and Correlation" (25 mentions) could suggest critiques of plot logic, while "Unoriginal" (15 mentions) points to a common reader complaint about repetitive or derivative elements. Terms like "Sueness" (16 mentions) and "Zimatar" (16 mentions) also highlight frustration with specific character traits or plot points.

## Question: Can we detect sarcasm or irony in reviews, and how does it relate to the overall rating?

Sarcasm and irony are very challenging concepts to detect. A rudimentary way of tackling such an issue like this could be further sentiment analysis. This would entail analyzing negative reviews that contain positive sentiment.

```{r}
sentiment_scores <- top_book_reviews %>%
  unnest_tokens(word, Review.Text) %>%            # Tokenize the review text
  inner_join(get_sentiments("bing"), by = "word", relationship = "many-to-many") %>%  # Join with sentiment lexicon
  count(Book.Title, Review.Stars, sentiment) %>%  # Count occurrences of positive/negative words
  spread(sentiment, n, fill = 0) %>%               # Spread sentiment into separate columns
  mutate(sentiment_score = positive - negative)     # Calculate sentiment score
# View sentiment scores
head(sentiment_scores)
```

From here, we can plot which books have the greatest sentiment score for reviews with less than 3 stars - which could direct us to reviews that have sarcasm/irony.

```{r, fig.width=18, fig.height=5}
sarcastic_books <- sentiment_scores %>%
  filter(Review.Stars < 3, sentiment_score > 0) 

# Print the resulting books
print(sarcastic_books)

top_n_books <- sarcastic_books %>%
  arrange(desc(sentiment_score)) %>%
  filter(!Book.Title %in% "Shoe Dog: A Memoir by the Creator of Nike") %>% # causes unknow issue with the plot
  head(10)

# Print the resulting top books
head(top_n_books)

ggplot(top_n_books, aes(x = reorder(Book.Title, sentiment_score), y = sentiment_score)) +
  geom_col(fill = darjeeling1[2], alpha=0.8) +
  labs(title = "Top 10 Books with Less Than 3 Stars and Positive Sentiment",
       x = "Book Title",
       y = "Sentiment Score") +
  coord_flip()
```

For these books to have 1 or 2 star reviews with such high sentiment scores definitely indicate that there is probably some form of expression where the intended meaning differs from the literal meaning of the words used. After seeing these results, all that is left is to read the reviews ourselves to gauge whether or not these reviews contain verbal irony.

```{r}
average_ratings <- reviews %>%
  group_by(Book.Title) %>%
  summarize(avg_rating = mean(Review.Stars))

head(average_ratings)

# Filter for sarcastic reviews
sarcastic_reviews <- sentiment_scores %>%
  filter(sentiment_score > 0, Review.Stars <= 2) 

# Create a sarcasm indicator in the sarcastic_reviews dataframe
sarcastic_reviews <- sarcastic_reviews %>%
  mutate(sarcasm = 1)

# Create a non-sarcastic data frame for comparison
non_sarcastic_reviews <- sentiment_scores %>%
  filter(!(sentiment_score > 0 & Review.Stars <= 2)) %>%
  mutate(sarcasm = 0)

# Combine both data frames
combined_reviews <- bind_rows(sarcastic_reviews, non_sarcastic_reviews)

# Merge with average ratings to include average book rating
final_analysis <- combined_reviews %>%
  group_by(Book.Title) %>%
  summarize(sentiment_score = mean(sentiment_score, na.rm = TRUE),
            sarcasm = first(sarcasm)) %>%
  mutate(avg_rating = average_ratings$avg_rating)
  
# Calculate correlation between sentiment score and average rating
head(final_analysis)

correlation_result <- cor(final_analysis$sentiment_score, final_analysis$avg_rating)
print(paste("Correlation between sentiment score and average book rating:", correlation_result))

# Run a linear regression analysis
model <- lm(avg_rating ~ sentiment_score + sarcasm, data = final_analysis)
summary(model)

ggplot(final_analysis, aes(x = sentiment_score, y = avg_rating, color = factor(sarcasm))) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Sentiment Score vs. Book Rating",
       x = "Sentiment Score",
       y = "Average Book Rating",
       color = "Sarcasm Indicator")
```

Based on what we see above, we note that: - Reviews marked as sarcastic tend to lower the average ratings despite having potentially positive sentiment - The p-value for the sarcasm coefficient is very low (4.32e-09), indicating that the effect of sarcasm on average book ratings is statistically significant. It's also important to note the very low R-squared value though, indicating other factors play a more substantial role in influencing book ratings. -Reviews that contain sarcastic remarks may mislead potential readers into thinking a book is being praised, when in reality, it is being criticized. This disconnect can lead to lower ratings as readers might find that the book does not meet their expectations.

In conclusion, it is fair to say that there MIGHT be some evidence of sarcastic reviews having an impact on the overall rating, but it is more likely that other variables are more important in predicting overall review scores.
